
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6.9. SLAM &#8212; Introduction to Robotics and Perception</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/style.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="6.10. Planning for Autonomous Driving." href="S65_driving_planning.html" />
    <link rel="prev" title="6.8. Sensing for Autonomous Vehicles" href="S63_driving_sensing.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-312077-7', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Robotics and Perception</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction to Robotics and Perception
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S10_introduction.html">
   1. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S11_intro_state.html">
     1.1. Representing State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S12_intro_actions.html">
     1.2. Robot Actions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S13_intro_sensing.html">
     1.3. Sensing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S14_intro_perception.html">
     1.4. Perception
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S15_intro_decision.html">
     1.5. Planning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S16_intro_learning.html">
     1.6. Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S20_sorter_intro.html">
   2. A Trash Sorting Robot
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S21_sorter_state.html">
     2.1. Modeling the World State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S22_sorter_actions.html">
     2.2. Actions for Sorting Trash
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S23_sorter_sensing.html">
     2.3. Sensors for Sorting Trash
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S24_sorter_perception.html">
     2.4. Perception
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S25_sorter_decision_theory.html">
     2.5. Decision Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S26_sorter_learning.html">
     2.6. Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S30_vacuum_intro.html">
   3. A Robot Vacuum Cleaner
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S31_vacuum_state.html">
     3.1. Modeling the State of the Vacuum Cleaning Robot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S32_vacuum_actions.html">
     3.2. Actions over time
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S33_vacuum_sensing.html">
     3.3. Dynamic Bayes Nets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S34_vacuum_perception.html">
     3.4. Perception with Graphical Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S35_vacuum_decision.html">
     3.5. Markov Decision Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S36_vacuum_RL.html">
     3.6. Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S40_logistics_intro.html">
   4. Warehouse Robots in 2D
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S41_logistics_state.html">
     4.1. Continuous State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S42_logistics_actions.html">
     4.2. Moving in 2D
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S43_logistics_sensing.html">
     4.3. Sensor Models with Continuous State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S44_logistics_perception.html">
     4.7. Localization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S45_logistics_planning.html">
     4.8. Planning for Logistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S46_logistics_learning.html">
     4.9. Some System Identification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S50_diffdrive_intro.html">
   5. A Mobile Robot With Simple Kinematics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S51_diffdrive_state.html">
     5.1. State Space for a Differential Drive Robot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S52_diffdrive_actions.html">
     5.4. Motion Model for the Differential Drive Robot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S53_diffdrive_sensing.html">
     5.5. Robot Vision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S54_diffdrive_perception.html">
     5.6. Computer Vision 101
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S55_diffdrive_planning.html">
     5.7. Path Planning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S56_diffdrive_learning.html">
     5.8. Deep Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="S60_driving_intro.html">
   6. Autonomous Vehicles
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="S61_driving_state.html">
     6.1. Planar Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S62_driving_actions.html">
     6.6. Kinematics for Driving
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S63_driving_sensing.html">
     6.8. Sensing for Autonomous Vehicles
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     6.9. SLAM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S65_driving_planning.html">
     6.10. Planning for Autonomous Driving.
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S66_driving_DRL.html">
     6.11. Deep Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S70_drone_intro.html">
   7. Autonomous Drones in 3D
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S71_drone_state.html">
     7.1. Moving in Three Dimensions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S72_drone_actions.html">
     7.2. Multi-rotor Aircraft
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S73_drone_sensing.html">
     7.3. Sensing for Drones
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S74_drone_perception.html">
     7.4. Visual SLAM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S75_drone_planning.html">
     7.5. Trajectory Optimization
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/S64_driving_perception.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/gtbook/robotics"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/gtbook/robotics/issues/new?title=Issue%20on%20page%20%2FS64_driving_perception.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/gtbook/robotics/main?urlpath=tree/S64_driving_perception.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poses-in-2d-and-3d">
   6.9.1. Poses in 2D and 3D
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iterative-closest-points">
   6.9.2. Iterative Closest Points
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-closest-points">
     6.9.2.1. Finding Closest Points
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-the-pairwise-transform">
     6.9.2.2. Estimating the Pairwise Transform
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poseslam">
   6.9.3. PoseSLAM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-poseslam-factor-graph">
   6.9.4. The PoseSLAM Factor Graph
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#map-inference-via-least-squares">
   6.9.5. MAP Inference via Least-Squares
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poseslam-is-nonlinear">
   6.9.6. PoseSLAM is Nonlinear !
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nonlinear-optimization-for-poseslam">
   6.9.7. Nonlinear Optimization for PoseSLAM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-with-gtsam">
   6.9.8. Optimization with GTSAM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#slam-with-landmarks">
   6.9.9. SLAM with Landmarks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-larger-slam-example">
   6.9.10. A Larger SLAM Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gtsam-101">
   6.9.11. GTSAM 101
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>SLAM</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poses-in-2d-and-3d">
   6.9.1. Poses in 2D and 3D
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#iterative-closest-points">
   6.9.2. Iterative Closest Points
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-closest-points">
     6.9.2.1. Finding Closest Points
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-the-pairwise-transform">
     6.9.2.2. Estimating the Pairwise Transform
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poseslam">
   6.9.3. PoseSLAM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-poseslam-factor-graph">
   6.9.4. The PoseSLAM Factor Graph
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#map-inference-via-least-squares">
   6.9.5. MAP Inference via Least-Squares
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#poseslam-is-nonlinear">
   6.9.6. PoseSLAM is Nonlinear !
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nonlinear-optimization-for-poseslam">
   6.9.7. Nonlinear Optimization for PoseSLAM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-with-gtsam">
   6.9.8. Optimization with GTSAM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#slam-with-landmarks">
   6.9.9. SLAM with Landmarks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-larger-slam-example">
   6.9.10. A Larger SLAM Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gtsam-101">
   6.9.11. GTSAM 101
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <p><a href="https://colab.research.google.com/github/gtbook/robotics/blob/master/S64_driving_perception.ipynb" target="_parent"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="tex2jax_ignore mathjax_ignore section" id="slam">
<h1><span class="section-number">6.9. </span>SLAM<a class="headerlink" href="#slam" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div><p>SLAM is “Simultaneous Localization and Mapping”, a key capability for mobile robots.</p>
</div></blockquote>
<p><strong>This Section is still in draft mode and was released for adventurous spirits (and TAs) only.</strong></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<div align='center'>
<img src='https://github.com/gtbook/robotics/blob/main/Art/steampunk/S64-Autonomous%20Vehicle%20with%20LIDAR%20and%20cameras-07.jpg?raw=1' style='height:256 width:100%'/>
</div>
</div></div>
</div>
<div class="section" id="poses-in-2d-and-3d">
<h2><span class="section-number">6.9.1. </span>Poses in 2D and 3D<a class="headerlink" href="#poses-in-2d-and-3d" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>SE(2) generalizes to SE(3).</p>
</div></blockquote>
<p>To represent the pose of a vehicle, recall that 2D poses
$T\doteq(x,y,\theta)$ form the Special Euclidean group $SE(2)$, and
can be represented by $3\times3$ matrix of the form</p>
<p>$$
T=\left[\begin{array}{cc|c}
\cos\theta &amp; -\sin\theta &amp; x\
sin\theta &amp; \cos\theta &amp; y\
\hline 0 &amp; 0 &amp; 1
\end{array}\right]=\left[\begin{array}{cc}
R &amp; t\
0 &amp; 1
\end{array}\right]
$$</p>
<p>with the $2\times1$ vector $t$
representing the position of the vehicle, and $R$ the $2\times2$
rotation matrix representing the vehicle’s orientation in the plane.</p>
<p>Note that this representation generalizes equally to three dimensions,
but of course $t$ will be a three-vector, and $R$ will be a $3\times3$
rotation matrix representing the 3DOF attitude of the vehicle. The
latter can be decomposed into roll, pitch, and yaw, if so desired,
but we will not need that until the next chapter.</p>
</div>
<div class="section" id="iterative-closest-points">
<h2><span class="section-number">6.9.2. </span>Iterative Closest Points<a class="headerlink" href="#iterative-closest-points" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>ICP is a seminal method to align 2 point clouds.</p>
</div></blockquote>
<p><strong>Iterative closest points</strong> or <strong>ICP</strong> is a method to align two point clouds, e.g., two successive LIDAR scans. Let us use superscripts $a$ and $b$ to distinguish the two point clouds, and the points therein. Under the assumption that we have a good initial estimate $\hat{T^a_b}$ for the relative pose $T^a_b$ between the two point clouds, we iterate between two steps:</p>
<ul class="simple">
<li><p>find closest point correspondences between the two clouds;</p></li>
<li><p>re-estimate the relative pose $\hat{T^a_b}$ between the two clouds.</p></li>
</ul>
<p>These two steps are iterated until convergence, hence the name. Below we  explain both steps in order.</p>
<div class="section" id="finding-closest-points">
<h3><span class="section-number">6.9.2.1. </span>Finding Closest Points<a class="headerlink" href="#finding-closest-points" title="Permalink to this headline">¶</a></h3>
<p>The first step is the easiest: for each point $P^a_j$ in the first point cloud, find the closest point $P^b_j$ in the second point cloud. Stated formally we have:</p>
<p>$$
P^b_j = \arg \min_{P^b} | P^b - P^a_j|^2
$$</p>
<p>where minimizing the square is just as good as minimizing the distance, because they are monotonically related. This is known as the <strong>nearest neighbor</strong> problem, and doing so for all points is the <strong>all nearest neighbors</strong> problem.</p>
<p>The brute force algorithm of iterating over all points in the second cloud can be quite slow, and indeed finding all nearest neighbors that way has quadratic complexity. However, very fast <em>approximate</em> nearest neighbor algorithms are available. Many of these use specialized data structures, such as “KD-trees” or “Oct-trees” (in 3D). While the details are out of scope, intuitively these data structures  recursively divide up the point clouds into sub-clouds, such that sub-clouds unlikely to contain the nearest neighbor can be quickly excluded. We build this data structure ones for the second cloud, and then use it for all nearest neighbor searches, leading to complexity which is approximately $O(N \log N$).</p>
</div>
<div class="section" id="estimating-the-pairwise-transform">
<h3><span class="section-number">6.9.2.2. </span>Estimating the Pairwise Transform<a class="headerlink" href="#estimating-the-pairwise-transform" title="Permalink to this headline">¶</a></h3>
<p>The second step is the more interesting one: given a set of closest point pairs ${(P^a_j, P^b_j)}$, how can we estimate from those the relative pose $\widehat{T^a_b}$ between two point clouds? This is known as the <strong>pose alignment</strong> problem.</p>
<p>Let us first assume that the two point clouds only differ by a rotation $R^a_b$. When this is the case, and assuming we have corresponding points $P^a$ and $P^b$, then each point $P^a$ in the first cloud can be expressed as a function of a point $P^b$ in the second cloud:</p>
<p>$$
P^a = R^a_b P^b
$$</p>
<p>One might be tempted to think that therefore</p>
<p>$$
R^a_b = P^a (P^b)^T
$$</p>
<p>but that is just silly (think about why!). Interestingly, though, if we form the matrix</p>
<p>$$
H = \sum_j P^a_j (P^b_j)^T
$$</p>
<p>by summing over at least 3 point pairs $(P^a_j, P^b_j)$, it turns out that the rotation matrix $\widehat{R^a_b}$ closest to $H$ in the least squares sense <em>is</em> the best possible estimate for the unknown rotation $R^a_b$. In addition, using the <em>singular value decomposition</em> $H=U\Lambda V^T$ from linear algebra, it is <em>very</em> easy to compute:</p>
<p>$$
\widehat{R^a_b} = U V^T
$$</p>
<p>Interesting aside: this problem is known as the <em>orthogonal Procrustes problem</em> and its solution via SVD has been known since 1966, from a paper by Peter Schönemann in a <em>psychology</em> journal.</p>
<p>The above solves the problem when there is only rotation, but it turns out that the best possible translation estimate will always align both <em>centroids</em> of the point clouds. Hence, when there is translation present, we simply compute the matrix $H$ from the <em>centered</em> points,</p>
<p>$$
H = \sum_j (P^a_j-C^a) (P^b_j-C^b)^T
$$</p>
<p>where the point cloud centroids $C^a$ and $C^b$ are computed as</p>
<p>$$
C^a = \frac{1}{N} \sum_j P^a_j\text{    and    }C^b = \frac{1}{N} \sum_j P^b_j.
$$</p>
<p>Given the estimated rotation $\widehat{R^a_b}$, the translation estimate $\widehat{t^a_b}$ can then be estimated from</p>
<p>$$
C^a = \widehat{R^a_b} C^b + \widehat{t^a_b},
$$</p>
<p>and the final relative pose estimate is given by $\widehat{T^a_b} =(\widehat{R^a_b}, \widehat{t^a_b})$. By the way, all of the above math is identical for both the 2D and 3D case.</p>
</div>
</div>
<div class="section" id="poseslam">
<h2><span class="section-number">6.9.3. </span>PoseSLAM<a class="headerlink" href="#poseslam" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>PoseSLAM is SLAM with pose priors and relative pose constraints only. We can derive those from Iterative Closest Points (ICP).</p>
</div></blockquote>
<p><strong>SLAM</strong> is <strong>Simultaneous Localization and Mapping</strong>.
In the SLAM problem the goal is to localize a robot using the information coming
from the robot’s sensors.
We have already covered the localization problem in chapter 5,
using both Markov localization and Monte Carlo localization.
The additional wrinkle in SLAM is that we do
<em>not</em> know the map a priori, and hence we have to infer the unknown map
simultaneously with the robot’s location with respect to the evolving map.</p>
<p><strong>PoseSLAM</strong> is a variant of SLAM that uses pose constraints as the
basic building block, and where we optimize over the unknown vehicles
poses. We do not explicitly optimize over a map: that is reconstructed
after the fact.</p>
<p>The PoseSLAM problem is:</p>
<blockquote>
<div><p>given a set of noisy relative measurements or <strong>pose constraints</strong>
$\tilde{T}<em>{ij}$, recover the optimal set of poses $T</em>{i}^{*}$ that
maximizes the posteriori probability, i.e., recover the MAP solution.</p>
</div></blockquote>
<p>In the case of mapping for autonomous driving, these relative
measurements can be derived from performing ICP between overlapping
scans. We can use GPS and/or IMU measurements to decide which scans
overlap, so that we do not have to compare $O(n^{2})$ scans. Depending
on the situation, we can optimize for 3D or 2D poses, in the way we will
discus below. Afterwards, we can reconstruct a detailed map by
transforming the local LIDAR scans into the world frame, using the
optimized poses $T_{i}^{*}$.</p>
</div>
<div class="section" id="the-poseslam-factor-graph">
<h2><span class="section-number">6.9.4. </span>The PoseSLAM Factor Graph<a class="headerlink" href="#the-poseslam-factor-graph" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Factor graphs expose the sparse set of constraints tying absolute poses together.</p>
</div></blockquote>
<p>In our factor-graph-based view of the world, a pose constraint is
represented as a factor. As before, the factor graph represent the
posterior distribution over the unknown pose variables
$\mathcal{T}={X_{1}\dots X_{5}}$ given the known measurements:</p>
<p>$$
\phi(\mathcal{T})=\prod_{i}\phi_{i}(\mathcal{T}_{i}),
$$</p>
<p>where $\mathcal{T}_{i}$ is the set of poses involved with factor $\phi_i$.
In this way, the factor graph encodes which factors are connected to which variables,
exposing the sparsity pattern of the corresponding estimation problem.</p>
<figure>
<img src="https://github.com/gtbook/robotics/blob/main/Figures6/PoseSLAM-FG.png?raw=1" id="fig:PoseSLAMFG" style="width:60.0%" /><figcaption><span id="fig:PoseSLAMFG" label="fig:PoseSLAMFG">[fig:PoseSLAMFG]</span> PoseSLAM factor graph example.</figcaption>
</figure>
<p>An example is shown in Figure
<a href="#fig:PoseSLAMFG" data-reference-type="ref" data-reference="fig:PoseSLAMFG">1</a>.
The example represents a vehicle driving around, and taking LIDAR scans
at 5 different world poses, represented by $T_{1}$ to $T_{5}$.
The factors $f_{1}$ to $f_{4}$ are binary factors representing the pose
constraints obtained by matching successive LIDAR scans using ICP.</p>
<p>The factor
$f_{5}(T_{5},T_{2})$ is a so-called <strong>loop closure</strong> constraint:
rather than derived from two successive scans, this one is derived from
matching the scan taken at $T_{5}$ with the one at $T_{2}$.
Detecting such loops can be done through a variety of means. The final,
unary factor $f_{0}(T_{1})$ is there to “anchor” the solution to the
origin: if it is not there, the solution will be undetermined. Another
way to anchor the solution is to add unary factors at every time-step,
derived from GPS.</p>
</div>
<div class="section" id="map-inference-via-least-squares">
<h2><span class="section-number">6.9.5. </span>MAP Inference via Least-Squares<a class="headerlink" href="#map-inference-via-least-squares" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Linear problems with zero-mean Gaussian noise are exactly least-squares.</p>
</div></blockquote>
<p>Finding the maximum a posteriori (MAP) solution in the case that variables are continuous and
measurements are linear combinations of them can be done via
least-squares optimization.
Earlier in this book we have discussed MAP inference for discrete
variables, and we have discussed probability distributions for
continuous variables, but we have never put the two together.</p>
<p>In the case of measurements corrupted by zero-mean Gaussian noise, we can
recover the MAP solution by minimizing a sum of square differences.
Recall that a multivariate Gaussian density <strong>with mean</strong> $\mu$ and <strong>variance</strong> $\sigma^{2}$ is
given by</p>
<p>$$
\mathcal{N} x\mu\sigma^2=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left{ -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right} .
$$</p>
<p>A simple 1D example can help explain this more clearly.
In particular, if we focus our attention in PoseSLAM on <em>just the x coordinates</em>, then we
predict relative measurements $\tilde{x}_{ij}$ by</p>
<p>$$
\tilde{x}<em>{ij}\approx h(x</em>{i,}x_{j})=x_{j}-x_{i}
$$
and each factor in
Figure
<a href="#fig:PoseSLAMFG" data-reference-type="ref" data-reference="fig:PoseSLAMFG">1</a>
could be written as</p>
<p>$$
\phi(x_{i},x_{j})=\frac{1}{\sqrt{2\pi}}\exp\left{ -\frac{1}{2}\left(x_{j}-x_{i}-\tilde{x}_{ij}\right)^{2}\right} ,
$$</p>
<p>where we assumed $\sigma=1$ for now. By taking the negative log,
maximizing the posterior corresponds to minimizing the following sum of
squares, where sum ranges over all $(i,j)$ pairs for which we have a
pairwise measurement:</p>
<p>$$
\mathcal{X}^{*}=\arg\min_{\mathcal{X}}\sum_{k}\frac{1}{2}\left(h(x_{i},x_{j})-\tilde{x}<em>{ij}\right)^{2}=\arg\min</em>{\mathcal{X}}\sum_{k}\frac{1}{2}\left(x_{j}-x_{i}-\tilde{x}_{ij}\right)^{2}.
$$</p>
<p>Linear least squares problems like these are easily solved by numerical
computing packages like MATLAB or numpy.</p>
</div>
<div class="section" id="poseslam-is-nonlinear">
<h2><span class="section-number">6.9.6. </span>PoseSLAM is Nonlinear !<a class="headerlink" href="#poseslam-is-nonlinear" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Nonlinear problems need nonlinear solutions.</p>
</div></blockquote>
<p>Unfortunately, in the PoseSLAM case we cannot use linear least squares,
because poses are not simply vectors, and the measurements are not
simply linear functions of the poses. Indeed, in PoseSLAM both the
prediction $h(T_{i},T_{j})$ and the measurement $\tilde{T}<em>{ij}$
are relative poses. The measurement prediction function $h(.)$ is given
by
$$
h(T</em>{i},T_{j})=T_{i}^{-1}T_{j}
$$
and the
measurement error to be minimized is</p>
<p>$$
\frac{1}{2}\left\Vert \log\left(\tilde{T}<em>{ij}^{-1}T</em>{i}^{-1}T_{j}\right)\right\Vert ^{2}
$$</p>
<p>where $\log:SE(2)\rightarrow\mathbb{R}^3$ denotes a map from $SE(2)$ to a
three-dimensional local coordinate vector $\xi$, which will be defined
in detail below.</p>
<p>There are two ways out of the nonlinear quandary. The first is to
realize that the only non-linearities stem from the $\sin$ and $\cos$
terms in the poses, associated with the unknown orientations
$\theta_{i}$. Hence, one solution is to try and solve for the
orientations first, and then solve for the translations using linear
least squares, exactly as above. This approach is known as <strong>rotation
averaging</strong> followed by linear translation recovery. Unfortunately it is
sub-optimal as it does not consider the orientation and translation
simultaneously. However, it can serve to provide a (very) good initial
estimate for nonlinear optimization, discussed below.</p>
<p>Indeed, we will prefer to take a second route, which is to use
<strong>nonlinear optimization</strong>, as discussed below.</p>
</div>
<div class="section" id="nonlinear-optimization-for-poseslam">
<h2><span class="section-number">6.9.7. </span>Nonlinear Optimization for PoseSLAM<a class="headerlink" href="#nonlinear-optimization-for-poseslam" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Linearize, solve, repeat…</p>
</div></blockquote>
<p>As discussed, the error expressions in PoseSLAM
are <em>nonlinear</em>, and we cannot directly optimize over the poses
$T_{i}$. Instead, we will locally linearize the problem and solve
the corresponding linear problem using least-squares, and iterate this
until convergence. We do this by, at each iteration, parameterizing a
pose $T$ by</p>
<p>$$
T\approx\bar{T}\Delta(\xi)
$$</p>
<p>where $\xi$ are local coordinates
$\xi\doteq(\delta x,\delta y,\delta\theta)$ and the incremental pose
$\Delta(\xi)\in SE(2)$ is defined as</p>
<p>$$
\Delta(\xi)=\left[\begin{array}{cc|c}
1 &amp; -\delta\theta &amp; \delta x\
\delta\theta &amp; 1 &amp; \delta y\
\hline 0 &amp; 0 &amp; 1
\end{array}\right]
$$</p>
<p>which you can recognize as a small angle
approximation of the $SE(2)$ matrix.
In 3D the local coordinates $\xi$ are 6-dimensional, and the small angle
approximation is defined as</p>
<p>$$
\Delta(\xi)=\left[\begin{array}{ccc|c}
1 &amp; -\delta\theta_{z} &amp; \delta\theta_{y} &amp; \delta x\
\delta\theta_{z} &amp; 1 &amp; -\delta\theta_{x} &amp; \delta y\
-\delta\theta_{y} &amp; \delta\theta_{x} &amp; 1 &amp; \delta z\
\hline 0 &amp; 0 &amp; 0 &amp; 1
\end{array}\right]
$$</p>
<p>With this new notation, we can approximate the
nonlinear error by a linear approximation:</p>
<p>$$
\frac{1}{2}\left\Vert \log\left(\tilde{T}<em>{ij}^{-1}T</em>{i}^{-1}T_{j}\right)\right\Vert ^{2}\approx\frac{1}{2}\left\Vert A_{i}\xi_{i}+A_{j}\xi_{j}-b\right\Vert ^{2}.
$$</p>
<p>For $SE(2)$ the matrices $A_{i}$ and $A_{j}$ are the $3\times3$ <strong>or
Jacobian matrices</strong> and $b$ is a $3\times1$ bias term. The above
provides a linear approximation of the term within the norm as a
function of the incremental local coordinates $\xi_{i}$ and $\xi_{j}$.
Deriving the detailed expressions for these Jacobians is beyond the
scope of this document, but suffice to say that they exist and not too
expensive to compute. In three dimensions, the Jacobian matrices are
$6\times6$ and $16\times6$, respectively.</p>
<p>The final optimization will—in each iteration—minimize over the local
coordinates of all poses by summing over all pose constraints. If we
index those constraints by $k$, we have the following least squares
problem:</p>
<p>$$
\Xi^{*}=\arg\min_{\Xi}\sum_{k}\frac{1}{2}\Vert A_{ki}\xi_{i}+A_{kj}\xi_{j}-b_{k}\Vert ^{2}
$$</p>
<p>where $\Xi\doteq {  \xi_{i}}$, the set
of all incremental pose coordinates.</p>
<p>After solving for the incremental updates $\Xi$, we update all poses
using the update equation above and check for convergence.
If the error does not decrease significantly
we terminate, otherwise we linearize and solve again, until the error
converges. While this is not guaranteed to converge to a global minimum,
in practice it does so if there are enough relative measurements and a
good initial estimate is available. For example, GPS can provide us with
a good initial estimate. However, especially in urban environments GPS
can be quite noisy, and it could happen that the map quality suffers by
converging to a bad local minimum. Hence, a good quality control process
is absolutely necessary in production environments.</p>
<p>In summary, the algorithm for nonlinear optimization is</p>
<ul class="simple">
<li><p>Start with an initial estimate $\mathcal{T}^{0}$</p></li>
<li><p>Iterate:</p>
<ol class="simple">
<li><p>Linearize the factors
$\frac{1}{2}\Vert \log(\tilde{T}<em>{ij}^{-1}T</em>{i}^{-1}T_{j})\Vert ^{2}\approx\frac{1}{2}\Vert A_{i}\xi_{i}+A_{j}\xi_{j}-b\Vert ^{2}$</p></li>
<li><p>Solve the least squares problem
$\Xi^{*}=\arg\min_{\Xi}\sum_{k}\frac{1}{2}\Vert A_{ki}\xi_{i}+A_{kj}\xi_{j}-b_{k}\Vert ^{2}$</p></li>
<li><p>Update $X_{i}^{t+1}\leftarrow X_{j}^{t}\Delta(\xi_{i})$</p></li>
</ol>
</li>
<li><p>Until the nonlinear error
$J(\mathcal{T})\doteq\sum_{k}\frac{1}{2}\Vert \log(\tilde{T}<em>{ij}^{-1}T</em>{i}^{-1}T_{j})\Vert ^{2}$
converges.</p></li>
</ul>
</div>
<div class="section" id="optimization-with-gtsam">
<h2><span class="section-number">6.9.8. </span>Optimization with GTSAM<a class="headerlink" href="#optimization-with-gtsam" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>GTSAM rocks PoseSLAM.</p>
</div></blockquote>
<p>For SLAM we typically use a specialized packages such as GTSAM that
exploit the sparsity of the factor graphs to dramatically
speed up computation.
GTSAM exploits sparsity to be computationally efficient. Typically
measurements only provide information on the relationship between a
handful of variables, and hence the resulting factor graph will be
sparsely connected. This is exploited by the algorithms implemented in
GTSAM to reduce computational complexity. Even when graphs are too dense
to be handled efficiently by direct methods, GTSAM provides iterative
methods that are quite efficient regardless.</p>
<p>The following code, included in GTSAM as an example, creates the
factor graph from Figure
<a href="#fig:PoseSLAMFG" data-reference-type="ref" data-reference="fig:PoseSLAMFG">1</a>
in code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">NonlinearFactorGraph</span><span class="p">()</span>
<span class="n">prior_model</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">noiseModel</span><span class="o">.</span><span class="n">Diagonal</span><span class="o">.</span><span class="n">Sigmas</span><span class="p">((</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">gtsam</span><span class="o">.</span><span class="n">PriorFactorPose2</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">prior_model</span><span class="p">))</span>

<span class="c1"># Create odometry (Between) factors between consecutive poses</span>
<span class="n">odometry_model</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">noiseModel</span><span class="o">.</span><span class="n">Diagonal</span><span class="o">.</span><span class="n">Sigmas</span><span class="p">((</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
<span class="n">Between</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">BetweenFactorPose2</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>

<span class="c1"># Add the loop closure constraint</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Lines 1-4 create a nonlinear factor graph and add the unary factor
$f_{0}(T_{1})$. As the vehicle travels through the world, it creates
binary factors $f_{t}(T_{t},T_{t+1})$ corresponding to odometry,
added to the graph in lines 6-12 (Note that M_PI_2 refers to pi/2).
But line 15 models a different event: a <strong>loop closure</strong>. For example,
the vehicle might recognize the same location using vision or a laser
range finder, and calculate the geometric pose constraint to when it
first visited this location. This is illustrated for poses $T_{5}$
and $T_{2}$, and generates the (red) loop closing factor
$f_{5}(T_{5},T_{2})$.</p>
<p>Before we can optimize, we need to create an initial estimate to start from. In GTSAM, this is done via the <code class="docutils literal notranslate"><span class="pre">gtsam.Values</span></code> type:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the initial estimate</span>
<span class="n">initial_estimate</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Values</span><span class="p">()</span>
<span class="n">initial_estimate</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">))</span>
<span class="n">initial_estimate</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">))</span>
<span class="n">initial_estimate</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">4.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>
<span class="n">initial_estimate</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span>
<span class="n">initial_estimate</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">initial_estimate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Values with 5 values:
Value 1: (gtsam::Pose2)
(0.5, 0, 0.2)

Value 2: (gtsam::Pose2)
(2.3, 0.1, -0.2)

Value 3: (gtsam::Pose2)
(4.1, 0.1, 1.5708)

Value 4: (gtsam::Pose2)
(4, 2, 3.14159)

Value 5: (gtsam::Pose2)
(2.1, 2.1, -1.5708)
</pre></div>
</div>
</div>
</div>
<p>We can use this initial estimate to show the factor graph below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">show</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">initial_estimate</span><span class="p">,</span> <span class="n">binary_edges</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S64_driving_perception_25_0.svg" src="_images/S64_driving_perception_25_0.svg" /></div>
</div>
<p>Optimization is done using non-linear minimization, as explained above. In GTSAM, this is done via a <code class="docutils literal notranslate"><span class="pre">NonlinearOptimizer</span></code> class. The specific optimizer we use below is <code class="docutils literal notranslate"><span class="pre">GaussNewtonOptimizer</span></code>, which exactly implements the pseudo-code given above, but exploiting sparsity in the factor graph to do this very efficiently. The optimizer only needs a graph and an initial estimate, both of which we already created, and hence the code below is quite simple:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Optimize the initial values using a Gauss-Newton nonlinear optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">GaussNewtonOptimizer</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">initial_estimate</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final Result:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Final Result:
Values with 5 values:
Value 1: (gtsam::Pose2)
(-2.76843e-20, -8.152e-20, -3.57721e-20)

Value 2: (gtsam::Pose2)
(2, -1.89295e-19, -5.34287e-20)

Value 3: (gtsam::Pose2)
(4, -3.42174e-11, 1.5708)

Value 4: (gtsam::Pose2)
(4, 2, 3.14159)

Value 5: (gtsam::Pose2)
(2, 2, -1.5708)
</pre></div>
</div>
</div>
</div>
<p>We can also inspect the result graphically. Looking at the result as printed above only gets us so far, and more importantly: it only shows us the maximum a posteriori (MAP) solution, but not the uncertainty around it. Luckily, GTSAM can also compute the <strong>posterior marginals</strong>, which show the uncertainty on each recovered pose as a Gaussian density $P(T_i|Z)$, taking into account all the measurements $Z$.</p>
<p>In code, we do this via the <code class="docutils literal notranslate"><span class="pre">gtsam.Marginals</span></code> object, and we can plot marginals with a special function <code class="docutils literal notranslate"><span class="pre">plot_pose2</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">marginals</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Marginals</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">):</span>
    <span class="n">gtsam_plot</span><span class="o">.</span><span class="n">plot_pose2</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">atPose2</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="mf">0.5</span><span class="p">,</span>
                          <span class="n">marginals</span><span class="o">.</span><span class="n">marginalCovariance</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S64_driving_perception_29_0.png" src="_images/S64_driving_perception_29_0.png" />
</div>
</div>
<p>The result is shown graphically in Figure
<a href="#fig:example" data-reference-type="ref" data-reference="fig:example">2</a>,
along with covariance ellipses shown in green. These covariance ellipses
in 2D indicate the marginal over position, <em>over all possible
orientations</em>, and show the area which contain 99% of the probability
mass (in 1D this would correspond to one standard deviation). The graph
shows in a clear manner that the uncertainty on pose $T_{5}$ is now
much less than if there would be only odometry measurements. The pose
with the highest uncertainty, $T_{4}$, is the one furthest away from
the unary constraint $f_{0}(T_{1})$, which is the only factor tying
the graph to a global coordinate frame.</p>
</div>
<div class="section" id="slam-with-landmarks">
<h2><span class="section-number">6.9.9. </span>SLAM with Landmarks<a class="headerlink" href="#slam-with-landmarks" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Take PoseSLAM, add landmarks.</p>
</div></blockquote>
<p>So far we optimized over one type of variable, but often we build a landmark map <em>simultaneously</em> with the trajectory, i.e., this is <em>true</em> SLAM. In the next chapter, we will more thoroughly examine the full 3d case, whereas here we will model landmarks with 2D points in the plane. That does not mean that they cannot be standings for real 3-D entities in the environment: they can be the location of trees, poles, building corners, the sides of windows, the location of a stop sign in traffic, even moving pedestrians in more advanced SLAM/tracking applications.</p>
<p>How do we measure such landmarks? The most typical <em>type</em> of measurements are either <strong>range</strong> measurements, <strong>bearing</strong> measurements, or <strong>bearing-range</strong> measurements of coverage combine the former two. The details on how to obtain them are typically application-dependent, and below we will abstract away the sensor pre-processing details. For example, in the case of a LiDAR sensors, bearing range measurements can be obtained by pre-processing the every LIDAR scan, detecting prominent vertical structures for example. A real-life example that we will discuss below involves detecting and measuring the bearing/range to trees. Another often-used sensors in autonomous driving includes radar, which can often be modeled or idealized to give bearing range measurements as well.</p>
<p>To illustrate SLAM with landmarks, we build a small toy example with 3 bearing-range measurements to two different landmarks:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">slam_graph</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">NonlinearFactorGraph</span><span class="p">()</span>
<span class="n">slam_graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">PriorFactorPose2</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="n">prior_model</span><span class="p">))</span>
<span class="n">slam_graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>
<span class="n">slam_graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Between</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="n">odometry_model</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add Range-Bearing measurements to two different landmarks L1 and L2</span>
<span class="n">measurement_model</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">noiseModel</span><span class="o">.</span><span class="n">Diagonal</span><span class="o">.</span><span class="n">Sigmas</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]))</span>
<span class="n">BR</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">BearingRangeFactor2D</span>
<span class="n">l</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">gtsam</span><span class="o">.</span><span class="n">symbol</span><span class="p">(</span><span class="s1">&#39;l&#39;</span><span class="p">,</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]}</span> <span class="c1"># name landmark variables</span>
<span class="n">slam_graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BR</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Rot2</span><span class="o">.</span><span class="n">fromDegrees</span><span class="p">(</span><span class="mi">45</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">4.0</span> <span class="o">+</span> <span class="mf">4.0</span><span class="p">),</span> <span class="n">measurement_model</span><span class="p">))</span> <span class="c1"># pose 1 -*- landmark 1</span>
<span class="n">slam_graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BR</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Rot2</span><span class="o">.</span><span class="n">fromDegrees</span><span class="p">(</span><span class="mi">90</span><span class="p">),</span> <span class="mf">2.0</span><span class="p">,</span><span class="n">measurement_model</span><span class="p">))</span> <span class="c1"># pose 2 -*- landmark 1</span>
<span class="n">slam_graph</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BR</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">l</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Rot2</span><span class="o">.</span><span class="n">fromDegrees</span><span class="p">(</span><span class="mi">90</span><span class="p">),</span> <span class="mf">2.0</span><span class="p">,</span><span class="n">measurement_model</span><span class="p">))</span> <span class="c1"># pose 3 -*- landmark 2</span>
</pre></div>
</div>
</div>
</div>
<p>When we have an initial estimate, we can look at the structure of this factor graph:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">slam_initial</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Values</span><span class="p">()</span>
<span class="n">slam_initial</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.20</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">))</span>
<span class="n">slam_initial</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">2.30</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.20</span><span class="p">))</span>
<span class="n">slam_initial</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose2</span><span class="p">(</span><span class="mf">4.10</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">))</span>
<span class="n">slam_initial</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point2</span><span class="p">(</span><span class="mf">1.80</span><span class="p">,</span> <span class="mf">2.10</span><span class="p">))</span>
<span class="n">slam_initial</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point2</span><span class="p">(</span><span class="mf">4.10</span><span class="p">,</span> <span class="mf">1.80</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">show</span><span class="p">(</span><span class="n">slam_graph</span><span class="p">,</span> <span class="n">slam_initial</span><span class="p">,</span> <span class="n">binary_edges</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S64_driving_perception_37_0.svg" src="_images/S64_driving_perception_37_0.svg" /></div>
</div>
<p>We optimize again with LM, and show the marginals on both robot position and landmarrs, as before:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">LevenbergMarquardtOptimizer</span><span class="p">(</span><span class="n">slam_graph</span><span class="p">,</span> <span class="n">slam_initial</span><span class="p">)</span>
<span class="n">slam_result</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">marginals</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Marginals</span><span class="p">(</span><span class="n">slam_graph</span><span class="p">,</span> <span class="n">slam_result</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]:</span>
    <span class="n">gtsam_plot</span><span class="o">.</span><span class="n">plot_pose2</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">slam_result</span><span class="o">.</span><span class="n">atPose2</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">marginals</span><span class="o">.</span><span class="n">marginalCovariance</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]:</span>
    <span class="n">gtsam_plot</span><span class="o">.</span><span class="n">plot_point2</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">slam_result</span><span class="o">.</span><span class="n">atPoint2</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="n">j</span><span class="p">]),</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">P</span><span class="o">=</span><span class="n">marginals</span><span class="o">.</span><span class="n">marginalCovariance</span><span class="p">(</span><span class="n">l</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S64_driving_perception_40_0.png" src="_images/S64_driving_perception_40_0.png" />
</div>
</div>
</div>
<div class="section" id="a-larger-slam-example">
<h2><span class="section-number">6.9.10. </span>A Larger SLAM Example<a class="headerlink" href="#a-larger-slam-example" title="Permalink to this headline">¶</a></h2>
<p>Below we optimize a piece of the (old) <a class="reference external" href="http://www-personal.acfr.usyd.edu.au/nebot/victoria_park.htm">Victoria park dataset</a>, which involves a truck driving through a park in Sydney, extracting the position of trees in the park from LIDAR scans, just as we discussed above. The factor graph for this example is created from file and shown below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">datafile</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">findExampleDataFile</span><span class="p">(</span><span class="s1">&#39;example.graph&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">noiseModel</span><span class="o">.</span><span class="n">Diagonal</span><span class="o">.</span><span class="n">Sigmas</span><span class="p">([</span><span class="mf">0.05</span><span class="p">,</span><span class="mf">0.05</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">180</span><span class="p">])</span>
<span class="p">[</span><span class="n">graph</span><span class="p">,</span><span class="n">initial</span><span class="p">]</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">load2D</span><span class="p">(</span><span class="n">datafile</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="n">show</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span><span class="n">initial</span><span class="p">,</span> <span class="n">binary_edges</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S64_driving_perception_43_0.svg" src="_images/S64_driving_perception_43_0.svg" /></div>
</div>
<p>This is a much larger factor graph than any we have encountered before, and we can distinguish several features:</p>
<ul class="simple">
<li><p>There is a prominent backbone of truck poses, connected via odometry measurements.</p></li>
<li><p>There are about 20 landmarks, some of which are seen briefly, others are seen for longer periods of time.</p></li>
<li><p>The graph is very sparsely connected, and hence optimization will still be quite fast.</p></li>
</ul>
<p>Optimizing with <code class="docutils literal notranslate"><span class="pre">gtsam.LevenbergMarquardtOptimizer</span></code>, again…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">initial_poses</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">extractPose2</span><span class="p">(</span><span class="n">initial</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">initial_poses</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">initial</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">initial</span><span class="o">.</span><span class="n">atPose2</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">retract</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,))))</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">LevenbergMarquardtOptimizer</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">initial</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">optimize</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Below the plot both the initial estimate, which was created by adding random noise on top of the ground truth, and the optimized trajectory:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">initial_poses</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">extractPose2</span><span class="p">(</span><span class="n">initial</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">initial_poses</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">initial_poses</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;initial&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">))</span>
<span class="n">final_poses</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">utilities</span><span class="o">.</span><span class="n">extractPose2</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">final_poses</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">final_poses</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;optimized&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_yaxes</span><span class="p">(</span><span class="n">scaleanchor</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span><span class="n">scaleratio</span> <span class="o">=</span> <span class="mi">1</span><span class="p">);</span> <span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S64_driving_perception_47_0.png" src="_images/S64_driving_perception_47_0.png" />
</div>
</div>
</div>
<div class="section" id="gtsam-101">
<h2><span class="section-number">6.9.11. </span>GTSAM 101<a class="headerlink" href="#gtsam-101" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>A deeper dive in the GTSAM concepts used above.</p>
</div></blockquote>
<p>help(gtsam.Point3)</p>
<div class="highlight-Point3(x=nan, notranslate"><div class="highlight"><pre><span></span>    Shim for the deleted Point3 type.
</pre></div>
</div>
<p>help(gtsam.Rot3.Ypr)</p>
<div class="highlight-Ypr(...) notranslate"><div class="highlight"><pre><span></span>    Ypr(y: float, p: float, r: float) -&gt; gtsam.gtsam.Rot3
</pre></div>
</div>
<p>help(gtsam.Pose3)</p>
<div class="highlight-Align(...) notranslate"><div class="highlight"><pre><span></span> |      Align(*args, **kwargs)
 |      Overloaded function.
 |      
 |      1. Align(abPointPairs: std::vector&lt;std::pair&lt;Eigen::Matrix&lt;double, 3, 1, 0, 3, 1&gt;, Eigen::Matrix&lt;double, 3, 1, 0, 3, 1&gt; &gt;, std::allocator&lt;std::pair&lt;Eigen::Matrix&lt;double, 3, 1, 0, 3, 1&gt;, Eigen::Matrix&lt;double, 3, 1, 0, 3, 1&gt; &gt; &gt; &gt;) -&gt; Optional[gtsam.gtsam.Pose3]
 |      
 |      2. Align(a: numpy.ndarray[numpy.float64[m, n]], b: numpy.ndarray[numpy.float64[m, n]]) -&gt; Optional[gtsam.gtsam.Pose3]
 |  
 |  Expmap(...) from builtins.PyCapsule
 |      Expmap(v: numpy.ndarray[numpy.float64[m, 1]]) -&gt; gtsam.gtsam.Pose3
 |  
 |  ExpmapDerivative(...) from builtins.PyCapsule
 |      ExpmapDerivative(xi: numpy.ndarray[numpy.float64[m, 1]]) -&gt; numpy.ndarray[numpy.float64[6, 6]]
 |  
 |  Logmap(...) from builtins.PyCapsule
 |      Logmap(pose: gtsam.gtsam.Pose3) -&gt; numpy.ndarray[numpy.float64[6, 1]]
 |  
 |  LogmapDerivative(...) from builtins.PyCapsule
 |      LogmapDerivative(xi: gtsam.gtsam.Pose3) -&gt; numpy.ndarray[numpy.float64[6, 6]]
 |  
 |  adjoint(...) from builtins.PyCapsule
 |      adjoint(xi: numpy.ndarray[numpy.float64[m, 1]], y: numpy.ndarray[numpy.float64[m, 1]]) -&gt; numpy.ndarray[numpy.float64[6, 1]]
 |  
 |  adjointMap(...) from builtins.PyCapsule
 |      adjointMap(xi: numpy.ndarray[numpy.float64[m, 1]]) -&gt; numpy.ndarray[numpy.float64[6, 6]]
 |  
 |  adjointMap_(...) from builtins.PyCapsule
 |      adjointMap_(xi: numpy.ndarray[numpy.float64[m, 1]]) -&gt; numpy.ndarray[numpy.float64[6, 6]]
 |  
 |  adjointTranspose(...) from builtins.PyCapsule
 |      adjointTranspose(xi: numpy.ndarray[numpy.float64[m, 1]], y: numpy.ndarray[numpy.float64[m, 1]]) -&gt; numpy.ndarray[numpy.float64[6, 1]]
 |  
 |  adjoint_(...) from builtins.PyCapsule
 |      adjoint_(xi: numpy.ndarray[numpy.float64[m, 1]], y: numpy.ndarray[numpy.float64[m, 1]]) -&gt; numpy.ndarray[numpy.float64[6, 1]]
 |  
 |  identity(...) from builtins.PyCapsule
 |      identity() -&gt; gtsam.gtsam.Pose3
 |  
 |  wedge(...) from builtins.PyCapsule
 |      wedge(wx: float, wy: float, wz: float, vx: float, vy: float, vz: float) -&gt; numpy.ndarray[numpy.float64[m, n]]
 |  
 |  ----------------------------------------------------------------------
 |  Static methods inherited from pybind11_builtins.pybind11_object:
 |  
 |  __new__(*args, **kwargs) from pybind11_builtins.pybind11_type
 |      Create and return a new object.  See help(type) for accurate signature.

</pre></div>
</div>
<p>help(driving.read_lidar_points)</p>
<p>help(driving.visualize_clouds)
help(driving.visualize_clouds_animation)</p>
<p>help(gtsam.Values.deserialize)</p>
<p>help(gtsam.Pose3.transformFrom)</p>
<p>help(gtsam.noiseModel.Diagonal)</p>
<p>help(gtsam.NonlinearFactorGraph())</p>
<p>help(gtsam.PriorFactorPose3)
help(gtsam.BetweenFactorPose3)</p>
<p>help(gtsam.GaussNewtonParams)</p>
<p>help( gtsam.GaussNewtonOptimizer)</p>
<p>help(gtsam.Marginals)</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="S63_driving_sensing.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">6.8. </span>Sensing for Autonomous Vehicles</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="S65_driving_planning.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6.10. </span>Planning for Autonomous Driving.</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Frank Dellaert and Seth Hutchinson<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>