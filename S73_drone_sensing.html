
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>7.3. Sensing for Drones &#8212; Introduction to Robotics and Perception</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/style.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="7.4. Visual SLAM" href="S74_drone_perception.html" />
    <link rel="prev" title="7.2. Multi-rotor Aircraft" href="S72_drone_actions.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-312077-7', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Robotics and Perception</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction to Robotics and Perception
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S10_introduction.html">
   1. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S11_intro_state.html">
     1.1. Representing State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S12_intro_actions.html">
     1.2. Robot Actions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S13_intro_sensing.html">
     1.3. Sensing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S14_intro_perception.html">
     1.4. Perception
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S15_intro_decision.html">
     1.5. Planning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S16_intro_learning.html">
     1.6. Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S20_sorter_intro.html">
   2. A Trash Sorting Robot
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S21_sorter_state.html">
     2.1. Modeling the World State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S22_sorter_actions.html">
     2.2. Actions for Sorting Trash
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S23_sorter_sensing.html">
     2.3. Sensors for Sorting Trash
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S24_sorter_perception.html">
     2.4. Perception
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S25_sorter_decision_theory.html">
     2.5. Decision Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S26_sorter_learning.html">
     2.6. Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S30_vacuum_intro.html">
   3. A Robot Vacuum Cleaner
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S31_vacuum_state.html">
     3.1. Modeling the State of the Vacuum Cleaning Robot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S32_vacuum_actions.html">
     3.2. Actions over time
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S33_vacuum_sensing.html">
     3.3. Dynamic Bayes Nets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S34_vacuum_perception.html">
     3.4. Perception with Graphical Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S35_vacuum_decision.html">
     3.5. Markov Decision Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S36_vacuum_RL.html">
     3.6. Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S40_logistics_intro.html">
   4. Warehouse Robots in 2D
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S41_logistics_state.html">
     4.1. Continuous State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S42_logistics_actions.html">
     4.2. Moving in 2D
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S43_logistics_sensing.html">
     4.3. Sensor Models with Continuous State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S44_logistics_perception.html">
     4.4. Localization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S45_logistics_planning.html">
     4.5. Planning for Logistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S46_logistics_learning.html">
     4.6. Some System Identification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S50_diffdrive_intro.html">
   5. A Mobile Robot With Simple Kinematics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S51_diffdrive_state.html">
     5.1. State Space for a Differential Drive Robot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S52_diffdrive_actions.html">
     5.2. Motion Model for the Differential Drive Robot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S53_diffdrive_sensing.html">
     5.3. Robot Vision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S54_diffdrive_perception.html">
     5.4. Computer Vision 101
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S55_diffdrive_planning.html">
     5.5. Path Planning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S56_diffdrive_learning.html">
     5.6. Deep Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S60_driving_intro.html">
   6. Autonomous Vehicles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S61_driving_state.html">
     6.1. Planar Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S62_driving_actions.html">
     6.2. Kinematics for Driving
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S63_driving_sensing.html">
     6.3. Sensing for Autonomous Vehicles
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S64_driving_perception.html">
     6.4. SLAM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S65_driving_planning.html">
     6.5. Planning for Autonomous Driving.
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S66_driving_DRL.html">
     6.6. Deep Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="S70_drone_intro.html">
   7. Autonomous Drones in 3D
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="S71_drone_state.html">
     7.1. Moving in Three Dimensions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S72_drone_actions.html">
     7.2. Multi-rotor Aircraft
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     7.3. Sensing for Drones
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S74_drone_perception.html">
     7.4. Visual SLAM
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/S73_drone_sensing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/gtbook/robotics"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/gtbook/robotics/issues/new?title=Issue%20on%20page%20%2FS73_drone_sensing.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/gtbook/robotics/main?urlpath=tree/S73_drone_sensing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inertial-measurement-units">
   7.3.1. Inertial Measurement Units
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gyroscopes">
     7.3.1.1. Gyroscopes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#accelerometers">
     7.3.1.2. Accelerometers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#magnetometers">
     7.3.1.3. Magnetometers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cameras">
   7.3.2. Cameras
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#projecting-3d-points">
   7.3.3. Projecting 3D Points
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-stereo-example-in-code">
   7.3.4. A Stereo Example in Code
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Sensing for Drones</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inertial-measurement-units">
   7.3.1. Inertial Measurement Units
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gyroscopes">
     7.3.1.1. Gyroscopes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#accelerometers">
     7.3.1.2. Accelerometers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#magnetometers">
     7.3.1.3. Magnetometers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cameras">
   7.3.2. Cameras
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#projecting-3d-points">
   7.3.3. Projecting 3D Points
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-stereo-example-in-code">
   7.3.4. A Stereo Example in Code
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <p><a href="https://colab.research.google.com/github/gtbook/robotics/blob/main/S73_drone_sensing.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<div class="tex2jax_ignore mathjax_ignore section" id="sensing-for-drones">
<h1><span class="section-number">7.3. </span>Sensing for Drones<a class="headerlink" href="#sensing-for-drones" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div><p>Drones like inertial sensing and cameras.</p>
</div></blockquote>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<div align='center'>
<img src='https://github.com/gtbook/robotics/blob/main/Art/steampunk/S73-Autonomous%20camera%20drone-03.jpg?raw=1' style='height:256 width:100%'/>
</div>
</div></div>
</div>
<div class="section" id="inertial-measurement-units">
<h2><span class="section-number">7.3.1. </span>Inertial Measurement Units<a class="headerlink" href="#inertial-measurement-units" title="Permalink to this headline">¶</a></h2>
<p>Inertial measurement units (IMUs) measure how a rigid body moves through space, and have undergone revolutionary transformations in the last century. Whereas they were initially developed for military and later civilian navigation purposes, miniaturized (MEMS) versions of them are now built into most of the mobile computing devices people use every day. IMUs often bundle the following three sensors: gyroscopes, accelerometers, and magnetometers, which we discuss in turn below.</p>
<div class="section" id="gyroscopes">
<h3><span class="section-number">7.3.1.1. </span>Gyroscopes<a class="headerlink" href="#gyroscopes" title="Permalink to this headline">¶</a></h3>
<p>A <strong>gyroscope</strong> measures <em>changes</em> in orientation around three orthogonal axes, i.e., an angular velocity <span class="math notranslate nohighlight">\(\omega\)</span>.
This is useful for drones, as they fly in 3D space and have to carefully control and hence measure their attitude <span class="math notranslate nohighlight">\(R^n_b\)</span>. Gyroscopes do not directly measure attitude however, only the rate of change in orientation, and hence we need to integrate the angular velocity <span class="math notranslate nohighlight">\(\omega(t)\)</span> over time:</p>
<div class="math notranslate nohighlight">
\[
R^n_b(t) = R^n_b(0) \int_{\tau=0}^t \exp \hat{\omega}(\tau) d\tau
\]</div>
<p>Above, the exponential map <span class="math notranslate nohighlight">\(\exp \hat{\omega}(\tau)\)</span> is as defined in the previous section, where we have also shown how to numerically integrate forward in rotation space.</p>
<p>Unfortunately, when the gyroscope measurements are corrupted by random noise, this noise is also integrated over time. In addition, gyroscopes also suffer from <strong>bias</strong>, a non-zero offset of the measurements that changes slowly over time. If we know this bias, e.g., by estimating it, then we can subtract it first. However, if we do <em>not</em> know the bias, the resulting error grows linearly over time when it is integrated.</p>
<p>Both effects mean that we will gradually lose track of the correct attitude <span class="math notranslate nohighlight">\(R^n_b(t)\)</span>, a process known as “drift”.
Good (expensive) gyroscopes are able to track the attitude for a long time, whereas cheaper (MEMS) gyroscopes as those found in drones (and phones) can drift away from a usable attitude estimate in 100s or even 10s of seconds.</p>
</div>
<div class="section" id="accelerometers">
<h3><span class="section-number">7.3.1.2. </span>Accelerometers<a class="headerlink" href="#accelerometers" title="Permalink to this headline">¶</a></h3>
<p>An <strong>accelerometer</strong> measures the linear acceleration in 3D. While GPS, provided one is outdoors and in an open area, can provide a measurement of the absolute position <span class="math notranslate nohighlight">\(r^n(t)\)</span> of a drone, the errors associated with GPS are often large. Accelerometers are much more precise, but unfortunately do not provide absolute position: because they measure <em>forces</em> exerted upon them, any measurement they provide is essentially an acceleration, i.e., the second derivative of position. Hence, of course, the name “accelerometer”.</p>
<p>In theory, we can <em>doubly</em> integrate the measured acceleration to obtain the position <span class="math notranslate nohighlight">\(r^n(t)\)</span>.
However, because of the double integration, the effect of random noise and bias error is doubly compounded, making the use of an accelerometer for estimating a drone’s position rather tricky. It can be done, but it requires great care and careful initialization of the biases. In fact, aircraft equipped with <em>inertial navigation systems</em> typically have an “INS alignment procedure” where the aircraft remains stationary on the taxi-way for a short period prior to take-off. Even then, unless the accelerometer can be augmented with absolute sources of position, such as GPS, an INS is bound to diverge sooner or later.</p>
<p>One of the most frequent and dependable uses of an accelerometer is to “aid” a gyroscope, maintaining absolute orientation over time.
As discussed above, integrating the angular velocity <span class="math notranslate nohighlight">\(\omega(t)\)</span> over time accumulates error.
Because gravity is such a strong signal it often dwarfs the accelerations due to maneuvering, and hence we can use it to correct our attitude estimate <span class="math notranslate nohighlight">\(\hat{R}^n_b\)</span>. This is known as “aiding” the gyroscope. Note that the attitude has <em>three</em> degrees of freedom, and the accelerometer can only correct two of them: pitch and roll. The absolute heading of the drone is still <em>unobservable</em>.</p>
</div>
<div class="section" id="magnetometers">
<h3><span class="section-number">7.3.1.3. </span>Magnetometers<a class="headerlink" href="#magnetometers" title="Permalink to this headline">¶</a></h3>
<p>A magnetometer measures a 3D vector that points along Earth’s local magnetic field.
The magnetic field roughly points to the magnetic north, although it really is 3-dimensional, and magnetometers measure the strength of these field in all three axes. For drones, it is often a noisy and rather unreliable sensor, especially indoors or in the presence of large metal structures. One drone application is inspecting bridges, and one can see that in such cases a magnetometer’s usefulness will be rather limited.</p>
<p>Still, as a magnetometer functions like a sophisticated compass, its main use is to “aid” the gyroscope, providing a measurement on the absolute heading, i.e., the last remaining degree of freedom. While noisy, over time it can provide enough signal to recover and maintain the heading of the drone.</p>
</div>
</div>
<div class="section" id="cameras">
<h2><span class="section-number">7.3.2. </span>Cameras<a class="headerlink" href="#cameras" title="Permalink to this headline">¶</a></h2>
<p>The second frequently used sensor for drones is a camera, or multiple cameras. Cameras are light-weight, cheap, and they provide some amazing capabilities, which we will discuss below and in the next section. They are also <em>passive</em>, in that unlike LIDARs, they do not send out energy into the environment. This has obvious benefits in terms of stealth, important in some applications, but also is less power-hungry. In drones, battery autonomy is one of the key design constraints and hence cameras are a very popular sensor for that reason alone.</p>
<p>By tracking features in the image(s) over time, cameras can provide relative motion measurements, i.e., <strong>visual odometry</strong>. If you have a pre-existing map of the environment, you can use a camera to <strong>localize</strong>, providing absolute orientation and position even without an IMU. <em>If</em> an IMU is available it can be used to track the high frequency <em>relative</em> motion of the drone, while the visual information provides a lower frequency but <em>absolute</em> measurement of the drone’s pose. In that way, IMU and camera measurements are perfectly complementary. In addition, if no map is available, cameras can be used to build a 3-D map of the environment in real time, using a paradigm called <strong>visual SLAM</strong>.</p>
<p>In this section we will concentrate on the extrinsic calibration of cameras and camera rigs.
We already discussed cameras as sensors in section 5.2, including their <em>intrinsic</em> calibration parameters such as focal length, image center, etc.
However, when using cameras on a drone, it is very important to know the relative position and orientation of the camera with respect to the drone’s body frame, the so called <strong>extrinsic calibration parameters</strong>, consisting of a position <span class="math notranslate nohighlight">\(t^b_c\)</span> and orientation <span class="math notranslate nohighlight">\(R^b_c\)</span> of the camera in the body frame.</p>
<p>We first need to specify the <em>position</em> of the camera(s) on the drone.
Recall that the drone <em>body coordinate frame</em> is forward-left-up (FLU), and hence this is how we need to think about where the camera is:
a camera towards the front of the drone will have a positive <span class="math notranslate nohighlight">\(X\)</span> value, etc. Below is a simple example with two cameras in front and one towards the back of the drone:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point3</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span> <span class="c1"># front-left</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point3</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span> <span class="c1"># front-right</span>
<span class="n">t3</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point3</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span> <span class="c1"># back</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">scatter_3d</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">t1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">t2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">t3</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">t1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">t2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">t3</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">z</span><span class="o">=</span><span class="p">[</span><span class="n">t1</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">t2</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">t3</span><span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_traces</span><span class="p">(</span><span class="n">axes</span><span class="p">(</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Pose3</span><span class="p">(),</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.08</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;F&quot;</span><span class="p">,</span><span class="s2">&quot;L&quot;</span><span class="p">,</span><span class="s2">&quot;U&quot;</span><span class="p">]))</span> <span class="c1"># add FLU drone body frame</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_yaxes</span><span class="p">(</span><span class="n">scaleanchor</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">scaleratio</span> <span class="o">=</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S73_drone_sensing_12_0.png" src="_images/S73_drone_sensing_12_0.png" />
</div>
</div>
<p>To specify the <em>orientation</em> <span class="math notranslate nohighlight">\(R^b_c\)</span> for each of the cameras, we need to remember that (a) the Z-axis points into the scene, and (b) the Y-axis points down. The easiest way to specify this is by using the <code class="docutils literal notranslate"><span class="pre">gtsam.Rot3</span></code> constructor that takes three column vectors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">F</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">bTc1</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose3</span><span class="p">(</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Rot3</span><span class="p">(</span><span class="o">-</span><span class="n">L</span><span class="p">,</span><span class="o">-</span><span class="n">U</span><span class="p">,</span><span class="n">F</span><span class="p">),</span> <span class="n">t1</span><span class="p">)</span>
<span class="n">bTc2</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose3</span><span class="p">(</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Rot3</span><span class="p">(</span><span class="o">-</span><span class="n">L</span><span class="p">,</span><span class="o">-</span><span class="n">U</span><span class="p">,</span><span class="n">F</span><span class="p">),</span> <span class="n">t2</span><span class="p">)</span>
<span class="n">bTc3</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose3</span><span class="p">(</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Rot3</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="o">-</span><span class="n">U</span><span class="p">,</span><span class="o">-</span><span class="n">F</span><span class="p">),</span> <span class="n">t3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can again use the <code class="docutils literal notranslate"><span class="pre">gtbook.drones.axes</span></code> function to ease displaying this with plotly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="o">.</span><span class="n">add_traces</span><span class="p">(</span><span class="n">axes</span><span class="p">(</span><span class="n">bTc1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span><span class="s2">&quot;Y1&quot;</span><span class="p">,</span><span class="s2">&quot;Z1&quot;</span><span class="p">]))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_traces</span><span class="p">(</span><span class="n">axes</span><span class="p">(</span><span class="n">bTc2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span><span class="s2">&quot;Y2&quot;</span><span class="p">,</span><span class="s2">&quot;Z2&quot;</span><span class="p">]))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_traces</span><span class="p">(</span><span class="n">axes</span><span class="p">(</span><span class="n">bTc3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;X3&quot;</span><span class="p">,</span><span class="s2">&quot;Y3&quot;</span><span class="p">,</span><span class="s2">&quot;Z3&quot;</span><span class="p">]))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S73_drone_sensing_16_0.png" src="_images/S73_drone_sensing_16_0.png" />
</div>
</div>
<p>Try to understand the code above, which made camera 1 and 2 look forward (<span class="math notranslate nohighlight">\(F\)</span>), creating a <em>forward-looking stereo pair</em>, and camera 3 look backwards (<span class="math notranslate nohighlight">\(-F\)</span>). Especially for visual odometry, which we will cover in the next section, having both forward and backward looking cameras is a good idea, yielding high quality estimates of the drone’s rotation. Cameras pointed to the side will often suffer from motion blur in forward flight mode, especially with close obstacles at high speed.</p>
</div>
<div class="section" id="projecting-3d-points">
<h2><span class="section-number">7.3.3. </span>Projecting 3D Points<a class="headerlink" href="#projecting-3d-points" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Transforming from navigation to drone to camera.</p>
</div></blockquote>
<p>We already know how to project points specified in the camera frame from chapter 5. However, for visual odometry or visual slam, an additional step is needed: we need to transform the 3-D points from the navigation frame into the camera frame. This does not only involve the camera extrinsics, but also the drone’s pose in the navigation frame itself. Let us start by reviewing the fundamental projection equation from chapter 5:</p>
<div class="math notranslate nohighlight">
\[
u = u_0 + f \frac{X^c}{Z^c} ~~~~ v = v_0 + f \frac{Y^c}{Z^c}.
\]</div>
<p>where <span class="math notranslate nohighlight">\(u_0\)</span>, <span class="math notranslate nohighlight">\(v_0\)</span>, and <span class="math notranslate nohighlight">\(f\)</span> are the <em>intrinsic</em> camera calibration parameters, and <span class="math notranslate nohighlight">\(P^c=(X^c,Y^c,Z^c)\)</span> are the coordinates of a 3D point in the <em>camera</em> coordinate frame, hence the superscript <span class="math notranslate nohighlight">\(c\)</span>. However, what if we are given the 3D coordinates <span class="math notranslate nohighlight">\(P^n=(X^n,Y^n,Z^n)\)</span> in the ENU navigation frame, rather than in the camera frame? Because the camera is mounted on the drone, we have to do this in two steps:</p>
<ul class="simple">
<li><p>convert from navigation to body frame: <span class="math notranslate nohighlight">\(P^b = (R^n_b)^T (P^n - t^n_b)\)</span></p></li>
<li><p>convert from body to camera frame: <span class="math notranslate nohighlight">\(P^c = (R^b_c)^T (P^b - t^b_c)\)</span></p></li>
</ul>
<p>where <span class="math notranslate nohighlight">\(T^n_b=(R^n_b,t^n_b)\)</span> is the drone’s FLU body pose with respect to the ENU navigation frame, and <span class="math notranslate nohighlight">\(T^b_c=(R^b_c,t^b_c)\)</span> is the camera extrinsics specified in that body frame. In case there are multiple cameras the first conversion needs to be done only once, but the second conversion will be camera specific.</p>
</div>
<div class="section" id="a-stereo-example-in-code">
<h2><span class="section-number">7.3.4. </span>A Stereo Example in Code<a class="headerlink" href="#a-stereo-example-in-code" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>It all works, folks!</p>
</div></blockquote>
<p>As an example, let us assume the drone is at position <span class="math notranslate nohighlight">\(t^n_b=(100, 300, 10)\)</span>, i.e., 10 meters high, flying north:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">E</span><span class="p">,</span><span class="n">N</span><span class="p">,</span><span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ntb</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point3</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">nRb</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Rot3</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="o">-</span><span class="n">E</span><span class="p">,</span><span class="n">U</span><span class="p">)</span> <span class="c1"># flying north, left of drone facing west</span>
<span class="n">nTb</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Pose3</span><span class="p">(</span><span class="n">nRb</span><span class="p">,</span> <span class="n">ntb</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s project a point <span class="math notranslate nohighlight">\(P^n=(103,310,12)\)</span> 10 meters in front of the drone (check this!) into the stereo pair. We make use of the GTSAM method <code class="docutils literal notranslate"><span class="pre">Pose3.TransformTo</span></code> to convert from navigation to body (once) and then from body to camera (twice):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wP</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point3</span><span class="p">(</span><span class="mi">103</span><span class="p">,</span><span class="mi">310</span><span class="p">,</span><span class="mi">12</span><span class="p">)</span>
<span class="n">bP</span> <span class="o">=</span> <span class="n">nTb</span><span class="o">.</span><span class="n">transformTo</span><span class="p">(</span><span class="n">wP</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;bP = </span><span class="si">{</span><span class="n">bP</span><span class="si">}</span><span class="s2"> in (F,L,U) body frame&quot;</span><span class="p">)</span>
<span class="n">c1P</span> <span class="o">=</span> <span class="n">bTc1</span><span class="o">.</span><span class="n">transformTo</span><span class="p">(</span><span class="n">bP</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;c1P = </span><span class="si">{</span><span class="n">c1P</span><span class="si">}</span><span class="s2"> in camera frame 1&quot;</span><span class="p">)</span>
<span class="n">c2P</span> <span class="o">=</span> <span class="n">bTc2</span><span class="o">.</span><span class="n">transformTo</span><span class="p">(</span><span class="n">bP</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;c2P = </span><span class="si">{</span><span class="n">c2P</span><span class="si">}</span><span class="s2"> in camera frame 2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bP = [10. -3.  2.] in (F,L,U) body frame
c1P = [ 3.05 -1.99  9.9 ] in camera frame 1
c2P = [ 2.95 -1.99  9.9 ] in camera frame 2
</pre></div>
</div>
</div>
</div>
<p>As you can see, the point in body coordinates is <span class="math notranslate nohighlight">\(10m\)</span> ahead, because the x-coordinate is <span class="math notranslate nohighlight">\(10\)</span> in the FLU body frame. Moreover, the points expressed in the two forward-looking camera frames are identical <em>except</em> for the X coordinates, which is exactly what we expect from a stereo rig. We can then apply the intrinsics to get the final image coordinates, for example using a <span class="math notranslate nohighlight">\(640\times 480\)</span> image and a focal length of <span class="math notranslate nohighlight">\(300\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">480</span><span class="p">,</span> <span class="mi">300</span>
<span class="n">u0</span><span class="p">,</span> <span class="n">v0</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">w</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">h</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
<span class="n">u1</span><span class="p">,</span> <span class="n">v1</span> <span class="o">=</span> <span class="n">u0</span> <span class="o">+</span> <span class="n">f</span> <span class="o">*</span> <span class="n">c1P</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">c1P</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">v0</span> <span class="o">+</span> <span class="n">f</span> <span class="o">*</span> <span class="n">c1P</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">c1P</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;u1,v1 = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">([</span><span class="n">u1</span><span class="p">,</span><span class="n">v1</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2"> in image 1&quot;</span><span class="p">)</span>
<span class="n">u2</span><span class="p">,</span> <span class="n">v2</span> <span class="o">=</span> <span class="n">u0</span> <span class="o">+</span> <span class="n">f</span> <span class="o">*</span> <span class="n">c2P</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">c2P</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">v0</span> <span class="o">+</span> <span class="n">f</span> <span class="o">*</span> <span class="n">c2P</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">c2P</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;u2,v2 = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">([</span><span class="n">u2</span><span class="p">,</span><span class="n">v2</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2"> in image 2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>u1,v1 = [412.4 179.7] in image 1
u2,v2 = [409.4 179.7] in image 2
</pre></div>
</div>
</div>
</div>
<p>Again, exactly what we expect for a stereo rig. In this case the disparity is <span class="math notranslate nohighlight">\(412.4-409.4=3\)</span> pixels, and if we plug that into the fundamental stereo equation from Section 5.2, with baseline <span class="math notranslate nohighlight">\(10cm\)</span> (check the extrinsics!), we indeed obtain that the point is at a depth of <span class="math notranslate nohighlight">\(10m\)</span>:</p>
<div class="math notranslate nohighlight">
\[
Z = B \frac{f}{d} = 0.1 \frac{300}{3} = 10
\]</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="S72_drone_actions.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">7.2. </span>Multi-rotor Aircraft</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="S74_drone_perception.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7.4. </span>Visual SLAM</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Frank Dellaert and Seth Hutchinson<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>