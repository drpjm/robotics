
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>5.3. Robot Vision &#8212; Introduction to Robotics and Perception</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/style.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="5.4. Computer Vision 101" href="S54_diffdrive_perception.html" />
    <link rel="prev" title="5.2. Motion Model for the Differential Drive Robot" href="S52_diffdrive_actions.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-312077-7', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Robotics and Perception</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction to Robotics and Perception
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S10_introduction.html">
   1. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S11_intro_state.html">
     1.1. Representing State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S12_intro_actions.html">
     1.2. Robot Actions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S13_intro_sensing.html">
     1.3. Sensing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S14_intro_perception.html">
     1.4. Perception
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S15_intro_decision.html">
     1.5. Planning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S16_intro_learning.html">
     1.6. Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S20_sorter_intro.html">
   2. A Trash Sorting Robot
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S21_sorter_state.html">
     2.1. Modeling the World State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S22_sorter_actions.html">
     2.2. Actions for Sorting Trash
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S23_sorter_sensing.html">
     2.3. Sensors for Sorting Trash
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S24_sorter_perception.html">
     2.4. Perception
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S25_sorter_decision_theory.html">
     2.5. Decision Theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S26_sorter_learning.html">
     2.6. Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S30_vacuum_intro.html">
   3. A Robot Vacuum Cleaner
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S31_vacuum_state.html">
     3.1. Modeling the State of the Vacuum Cleaning Robot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S32_vacuum_actions.html">
     3.2. Actions over time
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S33_vacuum_sensing.html">
     3.3. Dynamic Bayes Nets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S34_vacuum_perception.html">
     3.4. Perception with Graphical Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S35_vacuum_decision.html">
     3.5. Markov Decision Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S36_vacuum_RL.html">
     3.6. Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S40_logistics_intro.html">
   4. Warehouse Robots in 2D
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S41_logistics_state.html">
     4.1. Continuous State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S42_logistics_actions.html">
     4.2. Moving in 2D
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S43_logistics_sensing.html">
     4.3. Sensor Models with Continuous State
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S44_logistics_perception.html">
     4.4. Localization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S45_logistics_planning.html">
     4.5. Planning for Logistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S46_logistics_learning.html">
     4.6. Some System Identification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="S50_diffdrive_intro.html">
   5. A Mobile Robot With Simple Kinematics
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="S51_diffdrive_state.html">
     5.1. State Space for a Differential Drive Robot
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S52_diffdrive_actions.html">
     5.2. Motion Model for the Differential Drive Robot
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     5.3. Robot Vision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S54_diffdrive_perception.html">
     5.4. Computer Vision 101
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S55_diffdrive_planning.html">
     5.5. Path Planning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S56_diffdrive_learning.html">
     5.6. Deep Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S60_driving_intro.html">
   6. Autonomous Vehicles
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S61_driving_state.html">
     6.1. Planar Geometry
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S62_driving_actions.html">
     6.2. Kinematics for Driving
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S63_driving_sensing.html">
     6.3. Sensing for Autonomous Vehicles
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S64_driving_perception.html">
     6.4. SLAM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S65_driving_planning.html">
     6.5. Planning for Autonomous Driving.
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S66_driving_DRL.html">
     6.6. Deep Reinforcement Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="S70_drone_intro.html">
   7. Autonomous Drones in 3D
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="S71_drone_state.html">
     7.1. Moving in Three Dimensions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S72_drone_actions.html">
     7.2. Multi-rotor Aircraft
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S73_drone_sensing.html">
     7.3. Sensing for Drones
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S74_drone_perception.html">
     7.4. Visual SLAM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="S75_drone_planning.html">
     7.5. Trajectory Optimization
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/S53_diffdrive_sensing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/gtbook/robotics"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/gtbook/robotics/issues/new?title=Issue%20on%20page%20%2FS53_diffdrive_sensing.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/gtbook/robotics/main?urlpath=tree/S53_diffdrive_sensing.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cameras">
   5.3.1. Cameras
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cameras-for-robot-vision">
   5.3.2. Cameras for Robot Vision
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#camera-geometry">
   5.3.3. Camera Geometry
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#camera-calibration">
   5.3.4. Camera Calibration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pinhole-projection-equations">
   5.3.5. Pinhole Projection Equations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#camera-calibration-in-gtsam">
   5.3.6. Camera Calibration in GTSAM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#camera-field-of-view">
   5.3.7. Camera Field of View
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stereo-vision">
   5.3.8. Stereo Vision
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Robot Vision</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cameras">
   5.3.1. Cameras
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cameras-for-robot-vision">
   5.3.2. Cameras for Robot Vision
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#camera-geometry">
   5.3.3. Camera Geometry
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#camera-calibration">
   5.3.4. Camera Calibration
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pinhole-projection-equations">
   5.3.5. Pinhole Projection Equations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#camera-calibration-in-gtsam">
   5.3.6. Camera Calibration in GTSAM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#camera-field-of-view">
   5.3.7. Camera Field of View
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stereo-vision">
   5.3.8. Stereo Vision
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <p><a href="https://colab.research.google.com/github/gtbook/robotics/blob/main/S53_diffdrive_sensing.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<div class="tex2jax_ignore mathjax_ignore section" id="robot-vision">
<h1><span class="section-number">5.3. </span>Robot Vision<a class="headerlink" href="#robot-vision" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div><p>A camera is a super-sensor.</p>
</div></blockquote>
<p><strong>This Section is still in draft mode and was released for adventurous spirits (and TAs) only.</strong></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<div align='center'>
<img src='https://github.com/gtbook/robotics/blob/main/Art/steampunk/S53-Two-wheeled%20Toy%20Robot-02.jpg?raw=1' style='height:256 width:100%'/>
</div>
</div></div>
</div>
<div class="section" id="cameras">
<h2><span class="section-number">5.3.1. </span>Cameras<a class="headerlink" href="#cameras" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Introduction to cameras.</p>
</div></blockquote>
<p>Everyone knows what a camera is these days, and you probably have between 1 and 5 on your phone, depending on what model you have.</p>
<p>Historically, a <strong>Camera Obscura</strong>, literally “dark room”, showed people that focused <em>upside-down</em> images can be formed on a surface, provided the light rays coming from outside the room were constricted to a small “pinhole”. If you have never experienced this in real-life, it is a worthwhile experience to see this with your own eyes. One of the surprising but obvious properties of a camera obscura is that the images <em>move</em>: it really is <em>video obscura</em>.</p>
<p>The question then is how to capture these fleeting images. Da Vinci apparently wrote extensively about the using the camera obscura for drawing, and several 17th century painters may have used it in their painting process, the most famous of them being <a class="reference external" href="https://en.wikipedia.org/wiki/Johannes_Vermeer">Johannes Vermeer</a>.
The invention of <strong>photography</strong> (painting with light!) is usually credited to <a class="reference external" href="https://en.wikipedia.org/wiki/Nic%C3%A9phore_Ni%C3%A9pce">Niépce</a>, who used a light-sensitive material to capture the light around 1825. However, it was his partner <a class="reference external" href="https://en.wikipedia.org/wiki/Louis_Daguerre">Daguerre</a> who introduced photography to the world on a large scale via his <em>Daguerreotype</em> process, released into the public domain in 1839.</p>
<p>Since the 1990s, <strong>digital cameras</strong> have replaced cameras based on chemical emulsions, using CCDs (charged-coupled devices) or CMOS sensors as the underlying technology. Both sensor types capture photons in an array of picture elements or <strong>pixels</strong>. We will not discuss in detail how this devices work, but in essence both sensor types count how many photons fall onto each pixel’s area over a given time period. Below we discuss the more practical matter of the format in which images come to us, and how they can be used for robot vision.</p>
</div>
<div class="section" id="cameras-for-robot-vision">
<h2><span class="section-number">5.3.2. </span>Cameras for Robot Vision<a class="headerlink" href="#cameras-for-robot-vision" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Two sensors in one</p>
</div></blockquote>
<p>Cameras are amazing devices, and actually pack <em>two</em> sensors in one. First, a camera accurately measures the direction to points in space. Second, the 2D images formed on the sensor can be analyzed by computer vision algorithms to recognize objects and analyze the scene in front of the robot. In this section we focus on the mechanics, however, and leave algorithms for Section 5.4.</p>
<p>A pinhole by itself is rather amazing, as it renders the entire scene in front entirely <em>in focus</em>. However, it has a large drawback, in that it only lets in a tiny amount of light. The solution is to use a <strong>lens</strong>, which <em>collects</em> light over a larger diameter and <em>focuses</em> the light onto the image sensor. The upshot is that we can collect a lot more light (photons) in the same amount of time. The <em>downside</em> is that only part of the scene can be in focus at a given time - a phenomenon that leads to the “depth of field” of a camera: the (possibly narrow) area between where objects are tt close or too far to be in focus.</p>
<p>The most important properties associated with a digital camera are its <em>resolution</em>, typically specified as <span class="math notranslate nohighlight">\(W \times H\)</span> in pixels, its <em>focal length</em> <span class="math notranslate nohighlight">\(f\)</span>, also specified in pixels, and its <em>field of view</em> (FOV), typically specified in degrees (horizontal, vertical, or diagonal). The resolution is a property of the <em>sensor</em>, whereas focal length and field of view depend on the lens. We will investigate the relationships between these quantities in below, where we talk about te geometry of the camera.</p>
<p>In essence, we get access to images as multi-dimensional arrays. Expensive CCD cameras have three sensors, one per color channel (<strong>R</strong>ed), <strong>G</strong>reen, and <strong>B</strong>lue), and hence their raw output can be represented as three arrays of numbers that represent light levels in a specific frequency band, roughly corresponding to the same frequency bands that receptors in our eye are sensitive to. However, most cameras now have a <em>single</em> CMOS sensor with a color filter on top (called a Bayer pattern), and specialized algorithms that hallucinate three color channels. Actually, most cameras do a great deal more processing to improve the color and lighting; this sometimes gets in the way of algorithms that rely on measuring light exactly, but those are rather rare. In most cases, we are content to forget about all that and simply think of a (color) image as a <span class="math notranslate nohighlight">\(H \times W \times 3\)</span> array of numbers, where <span class="math notranslate nohighlight">\(H\)</span> is the height of the image, and <span class="math notranslate nohighlight">\(W\)</span> the width.</p>
<p>As an example, below we show an image on the left, taken by the differential drive robot on the right:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image_name</span> <span class="o">=</span> <span class="s2">&quot;LL_color_1201754063.387872.bmp&quot;</span>
<span class="n">ROW</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;&lt;img src=&quot;</span><span class="si">{</span><span class="n">FIG5</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">image_name</span><span class="si">}</span><span class="s1">?raw=1&quot; alt=&quot;Outdoor, beaten down path&quot;&gt;&#39;</span><span class="p">,</span>
     <span class="sa">f</span><span class="s1">&#39;&lt;img src=&quot;</span><span class="si">{</span><span class="n">FIG5</span><span class="si">}</span><span class="s1">/lagr-robot.jpg?raw=1&quot; alt=&quot;LAGR robot&quot; height=&quot;359&quot;&gt;&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table width="100%" class={cls}>

<tr>
<td style="text-align:left;"><img src="https://raw.githubusercontent.com/gtbook/robotics/main/Figures5/LL_color_1201754063.387872.bmp?raw=1" alt="Outdoor, beaten down path"></td>
<td style="text-align:left;"><img src="https://raw.githubusercontent.com/gtbook/robotics/main/Figures5/lagr-robot.jpg?raw=1" alt="LAGR robot" height="359"></td>
</tr></table>
</div></div>
</div>
<p>A python library, the <em>Python Imaging Library</em> or PIL provides some basic capabilities to deal with digital images. We can load images using the <code class="docutils literal notranslate"><span class="pre">PIL.Image</span></code> class, examine its dimensions, and create a numpy array view (you can also use <code class="docutils literal notranslate"><span class="pre">display</span></code> in a notebook to show it):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image</span> <span class="o">=</span> <span class="n">diffdrive</span><span class="o">.</span><span class="n">read_image</span><span class="p">(</span><span class="n">image_name</span><span class="p">)</span> <span class="c1"># locally: PIL.Image.open(image_name)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;resolution = </span><span class="si">{</span><span class="n">image</span><span class="o">.</span><span class="n">width</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">image</span><span class="o">.</span><span class="n">height</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">image_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;image_data.shape = </span><span class="si">{</span><span class="n">image_data</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">image_data</span><span class="p">[</span><span class="mi">383</span><span class="p">,</span><span class="mi">511</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>resolution = 512x384
image_data.shape = (384, 512, 3)
[82 55 57]
</pre></div>
</div>
</div>
</div>
<p>We see that the image width and height are <span class="math notranslate nohighlight">\(512\)</span> and <span class="math notranslate nohighlight">\(384\)</span>, respectively. But when we access the array with numpy, the first (slowest changing) dimension is the <em>height</em>, followed by the width and then the color dimension. Hence, the numpy array has to be indexed using the <span class="math notranslate nohighlight">\((\text{row},\text{column})\)</span> convention, after which you get the RGB value in the array, as shown in the last line of code above.</p>
<p>It is customary to use variables <span class="math notranslate nohighlight">\((i,j)\)</span> or <span class="math notranslate nohighlight">\((r,c)\)</span> to index pixels, where the latter is slightly preferred as it emphasizes the <em>row</em> and <em>column</em> semantics of these <em>integer</em> coordinates.</p>
</div>
<div class="section" id="camera-geometry">
<h2><span class="section-number">5.3.3. </span>Camera Geometry<a class="headerlink" href="#camera-geometry" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Points in 3D project to points in 2D.</p>
</div></blockquote>
<p>To turn a camera into a useful sensor, we need to describe its operation.
We already did so at a superficial level, bit especially the geometry involved needs more detail: exactly what light falls into what pixel?
The simplest model for geometric image formation is the <strong>pinhole camera model</strong>.
Imagine a three-dimensional, orthogonal coordinate frame centered at center of the lens.
Computer vision folks use a very specific camera convention which will make the math easy:</p>
<ul class="simple">
<li><p>the X-axis points to the <em>right</em>;</p></li>
<li><p>the Y-axis points <em>down</em>; and</p></li>
<li><p>the Z-axis points into the scene.</p></li>
</ul>
<p>When we express 3D points in the scene according to this convention, in a coordinate frame that is attached the the cameras, we speak of specifying an object in <em>camera coordinates</em>. For example, a 2 meter tall person, standing 5 meters away, and 3 meters to the left, would have be in between these two 3D coordinates:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feet</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point3</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mf">1.7</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># point at the feet of the person, 5 meters in front of camera, 3 meters to the left</span>
<span class="n">head</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Point3</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># point at the top of the head (note, Y = *minus* 2 meters)</span>
</pre></div>
</div>
</div>
</div>
<p>Note that we specify the feet in <em>camera coordinates</em>, and if we are holding the camera level at a height of 1.7 meters, the feet of the person will be 1.7 meters <em>below</em> the pinhole position.</p>
<p>Thinking back to the camera obscura example, the pinhole camera model specifies a 3D point <span class="math notranslate nohighlight">\((X,Y,Z)\)</span> in camera coordinates will be projected onto an image plane <em>behind</em> the camera:</p>
<div class="math notranslate nohighlight">
\[
X_I = - F \frac{X}{Z} ~~~~
Y_I = - F \frac{Y}{Z} ~~~~
Z_I = -F
\]</div>
<p>where <span class="math notranslate nohighlight">\(F\)</span> is the distance, in meters, from the image plane to the pinhole, i.e., the center of the lens. The following figure shows the geometry:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">F</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># meter</span>
<span class="kn">from</span> <span class="nn">gtbook.diffdrive</span> <span class="kn">import</span> <span class="n">axes</span><span class="p">,</span> <span class="n">plane</span><span class="p">,</span> <span class="n">ray</span><span class="p">,</span> <span class="n">show_3d</span>
<span class="n">show_3d</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">plane</span><span class="p">(</span><span class="o">-</span><span class="n">F</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">ray</span><span class="p">(</span><span class="n">feet</span><span class="p">,</span> <span class="o">-</span><span class="n">F</span><span class="p">),</span> <span class="n">ray</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="o">-</span><span class="n">F</span><span class="p">)]</span> <span class="o">+</span> <span class="n">axes</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S53_diffdrive_sensing_15_0.png" src="_images/S53_diffdrive_sensing_15_0.png" />
</div>
</div>
<p>However, it is not easy to debug algorithms with a true <em>upside down</em>  pinhole image. Instead, we can define a <em>virtual image plane</em> at a distance <span class="math notranslate nohighlight">\(F\)</span> <em>in front</em> of the pinhole, which is non-physical, but has the advantage that the image now appears right-side up. We simply have to reflect the projected coordinates:</p>
<div class="math notranslate nohighlight">
\[
X_V = F \frac{X}{Z} ~~~~
Y_V = F \frac{Y}{Z} ~~~~
Z_V = F
\]</div>
<p>The virtual image geometry is shown below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">show_3d</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">plane</span><span class="p">(</span><span class="n">F</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">ray</span><span class="p">(</span><span class="n">feet</span><span class="p">,</span> <span class="n">F</span><span class="p">),</span> <span class="n">ray</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">F</span><span class="p">)]</span> <span class="o">+</span> <span class="n">axes</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/S53_diffdrive_sensing_17_0.png" src="_images/S53_diffdrive_sensing_17_0.png" />
</div>
</div>
<p>The above still has the disadvantage that we still have to take into account the focal length <span class="math notranslate nohighlight">\(F\)</span> when doing the projection. Dividing by the focal length yields the fundamental <em>pinhole projection equation</em>:</p>
<div class="math notranslate nohighlight">
\[
x = \frac{X}{Z} ~~~~ y = \frac{Y}{Z}
\]</div>
<p>The dimensionless <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> coordinates are called the <strong>intrinsic camera coordinates</strong>, and can be taught of as the image of the scene in a virtual image plane situated at a focal length of 1.0. Note that the image origin at <span class="math notranslate nohighlight">\((x,y)=(0,0)\)</span> is the location where the <em>optical axis</em> (the blue Z-axis above) pierces the image plane. The intrinsic coordinates are in essence measuring a direction in space, but parameterized by a location in the virtual image plane rather than two angles.</p>
</div>
<div class="section" id="camera-calibration">
<h2><span class="section-number">5.3.4. </span>Camera Calibration<a class="headerlink" href="#camera-calibration" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>From intrinsic to sensor coordinates.</p>
</div></blockquote>
<p>Intrinsic coordinates are dimensionless, but what <em>pixels</em> in an image do they correspond to?
Also, when we project real-valued 3D coordinates in an image, we get <em>real-valued</em> intrinsic coordinates <span class="math notranslate nohighlight">\((x,y)\)</span>. How does that relate to integer pixel coordinates?
To translate from intrinsic coordinates to pixel coordinates, we introduce real-valued <strong>sensor coordinates</strong> <span class="math notranslate nohighlight">\((u,v)\)</span>, with the following conventions (try to draw this out for a <span class="math notranslate nohighlight">\(4\times3\)</span> image!):</p>
<ul class="simple">
<li><p>the top-left of the sensor corresponds to <span class="math notranslate nohighlight">\((u, v)=(0.0, 0.0)\)</span>;</p></li>
<li><p>the bottom-right of the sensor corresponds to <span class="math notranslate nohighlight">\((u, v)=(W, H)\)</span>.</p></li>
</ul>
<p>Some things to note:</p>
<ul class="simple">
<li><p>the vertical <span class="math notranslate nohighlight">\(v\)</span>-axis points <em>down</em>;</p></li>
<li><p>the units are in pixels (<em>fractional</em> pixels, if being precise);</p></li>
<li><p>we swapped the convention from <span class="math notranslate nohighlight">\((r,c)\Leftrightarrow(\text{row},\text{column})\)</span> to <span class="math notranslate nohighlight">\((u,v)\Leftrightarrow(\text{horizontal}, \text{vertical})\)</span>;</p></li>
<li><p>the middle of pixel <span class="math notranslate nohighlight">\((r, c)=(0, 0)\)</span> has sensor coordinates <span class="math notranslate nohighlight">\((u, v)=(0.5, 0.5)\)</span>;</p></li>
<li><p>the middle of pixel <span class="math notranslate nohighlight">\((r, c)=(H, W)\)</span> has sensor coordinates <span class="math notranslate nohighlight">\((u, v)=(W-0.5, H-0.5)\)</span>.</p></li>
</ul>
<p>The simplest <strong>camera calibration model</strong> is just a linear mapping, which is most appropriate for lenses with a small field of view. For this we need four parameters <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span>, <span class="math notranslate nohighlight">\(u_0\)</span>, and <span class="math notranslate nohighlight">\(v_0\)</span>, to convert from intrinsic coordinates <span class="math notranslate nohighlight">\((x,y)\)</span> to sensor coordinates <span class="math notranslate nohighlight">\((u,v)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
u &amp;= u_0 + \alpha x \\
v &amp;= v_0 + \beta y
\end{align*}
\end{split}\]</div>
<p>As an example, consider the <a class="reference external" href="https://www.flir.com/products/firefly-s/?model=FFY-U3-04S2C-C">FireFly S</a> machine vision camera, which has the following specifications:</p>
<ul class="simple">
<li><p>sensor: <a class="reference external" href="https://www.phase1vision.com/userfiles/product_files/imx273_287_296_297_flyer.pdf">Sony IMX297</a> (CMOS)</p></li>
<li><p>resolution: 728 x 544</p></li>
<li><p>pixel size: 6.9 <span class="math notranslate nohighlight">\(\mu m\)</span> (H) x 6.9 <span class="math notranslate nohighlight">\(\mu m\)</span> (V)</p></li>
<li><p>sensor size: 6.3mm diagonally (sanity-check this!)</p></li>
</ul>
<p>We typically expect the <em>image center</em>, corresponding to <span class="math notranslate nohighlight">\((x,y)=(0.0,0.0)\)</span>, to be close to <span class="math notranslate nohighlight">\((u_0,v_0)=(W/2,H/2)\)</span>.
For the sensor above this would be <span class="math notranslate nohighlight">\((u_0,v_0)=(364.0, 272.0)\)</span>.
To compute <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> we have to take into account the lens focal length <span class="math notranslate nohighlight">\(F\)</span>. Since <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span> are expressed in pixels, and <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are dimensionless, it is clear that <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> must also be expressed in pixels. They can be computed as</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha = F k = 8mm/6.9\mu m \approx 1160px\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta = F l = 8mm/6.9\mu m \approx 1160px\)</span></p></li>
</ul>
<p>where</p>
<div class="math notranslate nohighlight">
\[k = 1px/6.9\mu~~~~~\mathrm{and}~~~~l = 1px/6.9\mu\]</div>
<p>are sensor-specific constants that indicated the number of pixels per unit of length.</p>
<p>Whenever <span class="math notranslate nohighlight">\(k=l\)</span>, the sensor has <em>square pixels</em>, and we can just use one proportionality constant, <span class="math notranslate nohighlight">\(f=\alpha=\beta\)</span> where <span class="math notranslate nohighlight">\(f\)</span> is then also called the <em>focal length</em>, but expressed in pixels. This is a slight abuse of terminology, as <span class="math notranslate nohighlight">\(f\)</span> is a property of both the lens <em>and</em> the image sensor plane, but it is in widespread and we will adopt it here as well.</p>
</div>
<div class="section" id="pinhole-projection-equations">
<h2><span class="section-number">5.3.5. </span>Pinhole Projection Equations<a class="headerlink" href="#pinhole-projection-equations" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>The final frontier.</p>
</div></blockquote>
<p>We then finally have the <strong>fundamental pinhole projection equations</strong>, projecting a point <span class="math notranslate nohighlight">\(P\)</span> in 3D camera coordinates <span class="math notranslate nohighlight">\(P=(X,Y,Z)\)</span>, to its 2D image projection <span class="math notranslate nohighlight">\(p=(u,v)\)</span> in sensor coordinates:</p>
<div class="math notranslate nohighlight">
\[
u = u_0 + f \frac{X}{Z} ~~~~ v = v_0 + f \frac{Y}{Z}.
\]</div>
<p>To obtain integer pixel coordinates <span class="math notranslate nohighlight">\((r,c)\)</span>, we simply need to use the <em>floor</em> function, truncating the fractional pixel sensor coordinates to a location in the image array. Note that in doing so we also flip horizontal and vertical:</p>
<div class="math notranslate nohighlight">
\[
(r,c) = (\lfloor v \rfloor, \lfloor u \rfloor)
\]</div>
<p>We can also go the other way, <em>calibrating</em> the sensor coordinates <span class="math notranslate nohighlight">\((u,v)\)</span> to the dimensionless intrinsic coordinates <span class="math notranslate nohighlight">\((x,y)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
x &amp;= (u-u_0)/f \\
y &amp;= (v-v_0)/f
\end{align*}
\end{split}\]</div>
</div>
<div class="section" id="camera-calibration-in-gtsam">
<h2><span class="section-number">5.3.6. </span>Camera Calibration in GTSAM<a class="headerlink" href="#camera-calibration-in-gtsam" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Everything above and more.</p>
</div></blockquote>
<p>In GTSAM you have access to several calibration models, with the simple one above corresponding to <code class="docutils literal notranslate"><span class="pre">gtsam.Cal3_S2</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cal_8mm_FireFlyS</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">.</span><span class="n">Cal3_S2</span><span class="p">(</span><span class="n">fx</span><span class="o">=</span><span class="mi">1160</span><span class="p">,</span> <span class="n">fy</span><span class="o">=</span><span class="mi">1160</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">u0</span><span class="o">=</span><span class="mi">364</span><span class="p">,</span> <span class="n">v0</span><span class="o">=</span><span class="mi">272</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The arguments <code class="docutils literal notranslate"><span class="pre">fx</span></code> and <code class="docutils literal notranslate"><span class="pre">fy</span></code> above correspond to <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>, and for now you can ignore the extra <code class="docutils literal notranslate"><span class="pre">s</span></code> argument, denoting <em>skew</em> which almost always zero for modern sensors.
We can then convert from integer pixel coordinates to intrinsic coordinates:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calibration_demo</span><span class="p">(</span><span class="n">cal</span><span class="p">:</span><span class="n">gtsam</span><span class="o">.</span><span class="n">Cal3_S2</span><span class="p">,</span> <span class="n">row</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">col</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert from integer pixel coordinates to sensor and then intrinsic coordinates.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">row</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">col</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">u</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">+</span><span class="n">col</span><span class="p">,</span> <span class="mf">0.5</span><span class="o">+</span><span class="n">row</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">cal</span><span class="o">.</span><span class="n">calibrate</span><span class="p">([</span><span class="n">u</span><span class="p">,</span><span class="n">v</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;image[</span><span class="si">{</span><span class="n">row</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s2">] -&gt; (u,v)=(</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">px,</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">px) -&gt; (x,y)=(</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">) &quot;</span><span class="p">)</span>

<span class="n">calibration_demo</span><span class="p">(</span><span class="n">cal_8mm_FireFlyS</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">calibration_demo</span><span class="p">(</span><span class="n">cal_8mm_FireFlyS</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="mi">272</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">364</span><span class="p">)</span>
<span class="n">calibration_demo</span><span class="p">(</span><span class="n">cal_8mm_FireFlyS</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="mi">543</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">727</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>image[0,0] -&gt; (u,v)=(0.5px,0.5px) -&gt; (x,y)=(-0.313,-0.234) 
image[272,364] -&gt; (u,v)=(364.5px,272.5px) -&gt; (x,y)=(0.0,0.0) 
image[543,727] -&gt; (u,v)=(727.5px,543.5px) -&gt; (x,y)=(0.313,0.234) 
</pre></div>
</div>
</div>
</div>
<p>Note that although the intrinsic coordinates are the dimensionless, you can interpret them as fractions of the focal length.
Also, the above was a “calibration” example where we go from pixel coordinates to intrinsic coordinates. The calibration objects in GTSAM also provide an <code class="docutils literal notranslate"><span class="pre">uncalibrate</span></code> method which goes the other way:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">u</span><span class="p">,</span><span class="n">v</span> <span class="o">=</span> <span class="n">cal_8mm_FireFlyS</span><span class="o">.</span><span class="n">uncalibrate</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;(x,y)=(0,0) -&gt; (u,v)=(</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">px,</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">px)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(x,y)=(0,0) -&gt; (u,v)=(364.0px,272.0px)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="camera-field-of-view">
<h2><span class="section-number">5.3.7. </span>Camera Field of View<a class="headerlink" href="#camera-field-of-view" title="Permalink to this headline">¶</a></h2>
<p>The last concept we need the <strong>field of view</strong> or <strong>FOV</strong> of a camera.
Because the <em>left-most</em> ray we can see has <span class="math notranslate nohighlight">\(u=0\)</span>, it corresponds to <span class="math notranslate nohighlight">\(x=-u_0/f\approx-W/2f\)</span>.
The horizontal FOV can then be calculated by</p>
<div class="math notranslate nohighlight">
\[\mathrm{HFOV} = 2 \arctan(W/2f)~~\mathrm{rad} = 360 \arctan(W/2f) / \pi~~\mathrm{degrees}\]</div>
<p>For the sensor-lens combination above we get a relatively narrow field of view of about 35 degrees:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="mi">1160</span>
<span class="n">hfov</span> <span class="o">=</span> <span class="mi">360</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">atan</span><span class="p">(</span><span class="mi">728</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">f</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;HFOV for f=</span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">hfov</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> degrees&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>HFOV for f=1160 is 34.84 degrees
</pre></div>
</div>
</div>
</div>
<p>Field of view <em>increases</em> with decreasing focal length, e.g., a lens of 4mm will give us a bit less than double that HFOV, of around 64 degrees:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f_wide</span> <span class="o">=</span> <span class="mf">4e-3</span><span class="o">/</span><span class="mf">6.9e-6</span>
<span class="n">hfov_wide</span> <span class="o">=</span> <span class="mi">360</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">atan</span><span class="p">(</span><span class="mi">728</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">f_wide</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;HFOV for f=</span><span class="si">{</span><span class="n">f_wide</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">hfov_wide</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> degrees&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>HFOV for f=579.7 is 64.25 degrees
</pre></div>
</div>
</div>
</div>
<p>We can also ask the opposite question: what lens focal length should I choose to get a certain filed of view. For example, for a <em>diagonal</em> field of view we have</p>
<div class="math notranslate nohighlight">
\[\mathrm{DFOV} = 360 \arctan(\sqrt{W^2+H^2}/2f) / \pi~~\mathrm{degrees}\]</div>
<p>and hence</p>
<div class="math notranslate nohighlight">
\[f = \frac{\sqrt{W^2+H^2}}{2 \tan(\mathrm{DFOV} \pi/360)}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f45</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">728</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mi">544</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">4</span><span class="p">)</span>
<span class="n">F45</span> <span class="o">=</span> <span class="n">f45</span><span class="o">*</span><span class="mf">6.9e-3</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;f45 = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">f45</span><span class="p">)</span><span class="si">}</span><span class="s2"> pixels, F45 = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">F45</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2"> mm&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>f45 = 454.0 pixels, F45 = 3.1 mm
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="stereo-vision">
<h2><span class="section-number">5.3.8. </span>Stereo Vision<a class="headerlink" href="#stereo-vision" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Given two cameras, we can calculate depth.</p>
</div></blockquote>
<p>When using two cameras, we can triangulate features seen in both cameras to calculate its location in space.
Given a projection <span class="math notranslate nohighlight">\(p=(u,v)\)</span> of a point <span class="math notranslate nohighlight">\(P=(X,Y,Z)\)</span> in a single camera we can only determine the <em>ray</em> on which the point <span class="math notranslate nohighlight">\(P\)</span> must lie.
However, if we see <em>two</em> projections of the same feature in two cameras, placed side by side, we can <em>triangulate</em> the location of <span class="math notranslate nohighlight">\(P\)</span>.
In particular, let us name the cameras “Left” and “Right”, abbreviated as “L” and “R”, and let the two projections be <span class="math notranslate nohighlight">\(p_L=(u_L,v_L)\)</span> and <span class="math notranslate nohighlight">\(p_R=(u_R,v_R)\)</span>. How could we recover the coordinates <span class="math notranslate nohighlight">\((X,Y,Z)\)</span> in, say, the <em>left</em> camera coordinate frame?</p>
<p>We can easily work out the answer <em>if</em> the cameras have the same calibration <em>and</em> the camera pair is in a “stereo” configuration. The latter means that the cameras have exactly the same orientation with respect to the world, and the right camera is displaced only horizontally with respect to the left camera. We call the displacement the <strong>stereo baseline</strong> <span class="math notranslate nohighlight">\(B\)</span>. In that case we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
u_L &amp;= u_0 + f \frac{X}{Z}~~~~v_L = v_0 + f \frac{Y}{Z} \\
u_R &amp;= u_0 + f \frac{X-B}{Z}~~~~v_R = v_0 + f \frac{Y}{Z}
\end{align*}
\end{split}\]</div>
<p>Two interesting things to note: (a) <span class="math notranslate nohighlight">\(u_L\)</span> and <span class="math notranslate nohighlight">\(u_R\)</span> differ only because the <span class="math notranslate nohighlight">\(X\)</span> coordinate of the point <span class="math notranslate nohighlight">\(P\)</span>, measured in the right camera, is <span class="math notranslate nohighlight">\(B\)</span> less than its value in the left camera.
and (b) <span class="math notranslate nohighlight">\(v_L\)</span> and <span class="math notranslate nohighlight">\(v_R\)</span> have the same value: <em>corresponding</em> points in a stereo pair lie on the same scanline in the images. We can use the first fact to calculate the <em>unknown depth</em> <span class="math notranslate nohighlight">\(Z\)</span>, by defining the <strong>disparity</strong> <span class="math notranslate nohighlight">\(d\)</span> as the difference of <span class="math notranslate nohighlight">\(u_L\)</span> and <span class="math notranslate nohighlight">\(u_R\)</span>,</p>
<div class="math notranslate nohighlight">
\[
d \doteq u_L - u_R = f \frac{X}{Z} - f \frac{X-B}{Z},
\]</div>
<p>and then performing some algebraic manipulation to obtain the <strong>fundamental stereo equation</strong>:</p>
<div class="math notranslate nohighlight">
\[
Z = B \frac{f}{d}.
\]</div>
<p>The fraction <span class="math notranslate nohighlight">\(f/d\)</span> is dimensionless, as both disparity <span class="math notranslate nohighlight">\(d\)</span> and focal length <span class="math notranslate nohighlight">\(f\)</span> are expressed in pixels, and hence the resulting depth <span class="math notranslate nohighlight">\(Z\)</span> is expressed in the units of the baseline <span class="math notranslate nohighlight">\(B\)</span>. Using the other equation we can now completely reconstruct the location of the point <span class="math notranslate nohighlight">\(P\)</span> in (left) camera coordinates:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}X\\Y\\Z\end{bmatrix}
= \begin{bmatrix}Z(u_L-u_0)/f\\Z(v_L-v_0)/f\\B f/d\end{bmatrix}
= B \frac{f}{d} \begin{bmatrix}(u_L-u_0)/f\\(v_L-v_0)/f\\1\end{bmatrix}
\end{split}\]</div>
<p>Stereo cameras are used very often on robotics platforms because of this ability to reconstruct the world in 3D, at least in in principle. This is akin to our own (human) ability to perceive depth by virtue of having two eyes, a feature we have in common with many animals - primarily predators, who need accurate depth vision to hunt prey. In practice, using a stereo camera is not as straightforward, as it has to be carefully calibrated and finding <em>correspondences</em> between left and right cameras is not always straightforward. However, the latter has been alleviated quite a bit by recent advances in neural networks, which we will discuss in the next section.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="S52_diffdrive_actions.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">5.2. </span>Motion Model for the Differential Drive Robot</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="S54_diffdrive_perception.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5.4. </span>Computer Vision 101</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Frank Dellaert and Seth Hutchinson<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>